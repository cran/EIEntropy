<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Vignette EIEntropy</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Vignette EIEntropy</h1>


<div id="TOC">
<ul>
<li><a href="#methodology" id="toc-methodology">1. Methodology</a></li>
<li><a href="#data" id="toc-data">2. Data</a></li>
<li><a href="#examples" id="toc-examples">3. Examples</a></li>
<li><a href="#generalized-cross-entropy-ei_gce" id="toc-generalized-cross-entropy-ei_gce">3.1 Generalized Cross entropy:
ei_gce()</a></li>
<li><a href="#without-prior-information" id="toc-without-prior-information">3.2 Without prior
information</a></li>
<li><a href="#another-option-when-we-do-not-have-prior-information-ei_gme" id="toc-another-option-when-we-do-not-have-prior-information-ei_gme">3.2.1
Another option when we do not have prior information: ei_gme()</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>In a wide range of analyses, there is a common problem.  The data is
usually not available at the desired spatial scale that researchers
need.  Ecological inference allows us to recover incomplete or
unavailable data by inferring individual behaviours based on the
aggregated information. In some cases, the database containing the
variable of interest does not have it at the needed spatial level. 
Entropy allows us to obtain the best solution according to the
information that we have.  To learn more about the methodology, see
Fernández-Vázquez et al. (2020)</p>
<p>The package extrapolates the value of the variable of interest above
a data frame with more detailed geographical information, being
consistent with the aggregates available. This package contains two
functions, ei_gce() and ei_gme(). The method that the function ei_gce()
applies is framed between GCE (Generalized Cross entropy), minimising
the distance between the two distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, being this <span class="math inline">\(Q\)</span> the prior information we can initially
have.  If the user does not have prior information can choose ei_gme()
for computational reasons which will use GME (Generalized maximum
entropy) since our distribution a priori will be the uniform
distribution.  These two kinds of problems will be explained in
the next sections. </p>
<p>The EIentropy package makes the process of assessing uncertainty to
estimate information at the desired level of disaggregation into one
function. It gathers all the steps making it easy to apply this
methodology to problems of absence of data. This document introduces you
to the functions ei_gme() and ei_gce(), which apply entropy to solve
issues of ecological inference, providing, consistent disaggregated
indicators with observable aggregated data and cross moments.</p>
<p>Once you’ve installed it, the vignette (“EIEntropy”) will allow you
to learn more. We will use the data included in this package to explore
its functions.</p>
<p>This package is on CRAN and can be installed from within R.</p>
<div id="methodology" class="section level2">
<h2>1. Methodology</h2>
<p>The problem assessed is the need to obtain a variable <span class="math inline">\(Y\)</span> at a spatial level that is not
available. This variable <span class="math inline">\(Y\)</span> can take
<span class="math inline">\(J\)</span> values. The method aims to obtain
a matrix <span class="math inline">\(P\)</span> with dimension <span class="math inline">\(n\)</span> <em>x</em> <span class="math inline">\(J\)</span> being <span class="math inline">\(n\)</span> the number of observations. Matrix
<span class="math inline">\(P\)</span> is compounded by the
probabilities associated with each <span class="math inline">\(j\)</span> value for each observation. Taking
advantage of the information that we have, the method introduces the
cross-moments as a restriction to assure consistency.</p>
<p>Taking into account <span class="math inline">\(Y\)</span> is the sum
of <span class="math inline">\(P\)</span> and <span class="math inline">\(U\)</span>, being <span class="math inline">\(U\)</span> the random noise. Once we have
estimated matrix <span class="math inline">\(P\)</span> and the error
term, we can obtain <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y=P+U\)</span>. The error term is built as a
weighted mean of a support vector <span class="math inline">\(V\)</span>
in which weights <span class="math inline">\(W\)</span> are
estimated.</p>
<p>The support vector is the component of the noise defining the
flexibility of the estimation. It represents the maximum and minimum
error around a value in the center. By default, <span class="math inline">\(V\)</span> is defined as (var, 0, -var) where
<em>var</em> represents the variance of the dependent variable. If the
estimation requires more flexibility <span class="math inline">\(V\)</span> can be defined wider–with (1,0,-1) as
the recommended maximum.</p>
<p>We present two functions to apply ecological inference through
entropy. The general case minimizes the Kullback-Leibler divergence to
find the matrix of probabilities <span class="math inline">\(P\)</span>
with the minimum divergence with a prior. This case is adequate even
when we have prior information on the distribution of the probabilities
of our variable of interest. This information refers to all existing
knowledge previous to the data that can influence the expectations of
the variable of interest.</p>
<p>$$\min_{P} \, \text{KL}(P, Q) = \sum_{i=1}^{n} \sum_{j=1}^{J} p_{ij}
\log\left(\frac{p_{ij}}{q_{ij}}\right)+ \sum_{l=1}^{L} \sum_{i=1}^{n}
\sum_{j=1}^{J} w_{ijl} \log\left(\frac{w_{ijl}}{wo_{ijl}}\right)\quad
\text{s.t.}$$</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} x_{s_{ik}} y_{ij} = \frac{1}{n}
\sum_{i=1}^{n} x_{c_{ik}} (p_{ij} + u_{ij}) =\frac{1}{n} \sum_{i=1}^{n}
x_{cik} [p_{ij} + w_{Lij} v_{l}];         
j=1,\ldots,J
\]
\]</span> <span class="math display">\[\[
\sum_{j=1}^{J} p_{ij} = 1 \quad \text{for } i=1,\ldots,n
\]
\]</span> <span class="math display">\[\[
\sum_{l=1}^{L}  w_{Lij} = 1 \quad \text{for } i=1,\ldots,n;         
j=1,\ldots,J
\]
\]</span></p>
<p>This function minimizes the distance between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. Being <span class="math inline">\(Q\)</span> the prior information that we already
have. If we have some information about the possible distribution of our
variable of interest we can include this information here. For example,
if you know your variable of interest can take two values but one of
them is more common than the other you can include it in your estimation
by using <span class="math inline">\(Q\)</span>.</p>
<p>If we don’t have any information then we will assume there is the
same probability for each possibility <span class="math inline">\(j\)</span> of our variable of interest. In this
sense, we minimize the divergence with a uniform distribution.</p>
<p>Solving the optimization problem, we obtain estimations as <span class="math inline">\(\hat{Y}= \hat{P} + \hat{W}V\)</span>. The solution
should be consistent with the restrictions. In this case, the
restrictions will contain information from two data sources, datahp and
datahs. If there were divergences between the two sources of data, they
would be captured in the error term. This is one of the main advantages
of the methodology because it allows the user to use two databases with
divergences between them.</p>
</div>
<div id="data" class="section level2">
<h2>2. Data</h2>
<p>To explore the functions we will use data included in the package. In
our example, we want to obtain microdata about poverty, in terms of
wealth, with an indication of location at regional level. We have the
variable of interest in one official database but without information
about location. At the same time, in other database, the variable of
interest is not available but the household location is.</p>
<p>The data can be loaded by calling the functions financial() and
social().</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>datahp <span class="ot">&lt;-</span> <span class="fu">financial</span>()</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>datahs <span class="ot">&lt;-</span> <span class="fu">social</span>()</span></code></pre></div>
<p>In this example, we aim to obtain probabilities of being poor in
terms of wealth for each individual or observation in the survey with
detailed information about location. With this procedure, we will have
our variable of interest at the desired spatial scale. As it is known,
some variables, such as education level, income, or employment status
are related to wealth. Hence, we will use these variables as regressors
in the example. These variables are going to be our X. We have the same
variables in both data frames with the same name. For this example, the
financial survey includes 100 observations for the variables Dcollege,
Dunemp, total income and the variable of interest: poor_liq. They are a
dummy for college, a dummy for being unemployed, the household income
(in euros) and a dummy for being poor in terms of liquid assets
respectively.</p>
<p>The data called social has the same variables but instead of the
variable of interest, it has another variable with the region of each
household. In this database, we have 200 observations (households).</p>
</div>
<div id="examples" class="section level2">
<h2>3. Examples</h2>
</div>
<div id="generalized-cross-entropy-ei_gce" class="section level1">
<h1>3.1 Generalized Cross entropy: ei_gce()</h1>
<p>The function ei_gce() allows the user to introduce prior information
in the estimation. In this example, we keep using the databases included
in this package <em>financial</em> and <em>social</em>. Once we have
chosen the best function for our case note that we need to specify our
function :</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>fn <span class="ot">&lt;-</span> datahp<span class="sc">$</span>poor_liq <span class="sc">~</span> Dcollege<span class="sc">+</span>Totalincome<span class="sc">+</span>Dunemp</span></code></pre></div>
<p>Note that the same name in both datasets for the independent
variables is required.</p>
<p>This function’s arguments are the previously defined function, the
databases used, the weights, the tolerance, the maximum number of
iterations allowed and the support vector. With this function, weights
can be used and included with <em>weights</em>. If there are no weights
the function assumes a matrix of 1. Note that the weights used in this
methodology are normalised so analytics and sampling weights can be used
without distinction.</p>
<p>In this example the variable of interest (poor_liq) is defined with a
function in the argument <em>fn</em> (see previous section)</p>
<p>The arguments corresponding to the information a priori and the
support vector can be included as :</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>q <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.4</span>,<span class="fl">0.6</span>) </span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span>),<span class="at">nrow=</span><span class="dv">1</span>)</span></code></pre></div>
<p>In this example we assume a priori distribution of poverty equal to
0.4 for poor and 0.6 for non-poor. The support vector has been set to
the maximum (1,0,-1). Applying the ei_gce() we can solve the estimation
as:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">ei_gce</span>(fn,datahp,datahs,<span class="at">q=</span>q,<span class="at">weights =</span> w,<span class="at">v=</span>v)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>result</span></code></pre></div>
<pre><code>## $estimations
##     weights predictions_1 predictions_0  p_dual_1  p_dual_0     e_dual_1
## 1     0.005     0.3350458     1.0540023 0.3434272 0.6565728 -0.008381340
## 2     0.005     0.3867678     1.0012531 0.3658900 0.6341100  0.020877867
## 3     0.005     0.2932413     0.3059538 0.4972039 0.5027961 -0.203962630
## 4     0.005     0.2902375     0.8648859 0.3770933 0.6229067 -0.086855823
## 5     0.005     0.2615025     0.4771299 0.4533184 0.5466816 -0.191815842
## 6     0.005     0.2829758     0.5908130 0.4339745 0.5660255 -0.150998739
## 7     0.005     0.2846316     0.5993994 0.4325202 0.5674798 -0.147888668
## 8     0.005     0.3418405     1.0801222 0.3385628 0.6614372  0.003277746
## 9     0.005     0.2936922     0.8805166 0.3743905 0.6256095 -0.080698255
## 10    0.005     0.3095488     0.6461541 0.4279908 0.5720092 -0.118441957
## 11    0.005     0.3315515     1.0403106 0.3459516 0.6540484 -0.014400190
## 12    0.005     0.2520269     0.4257540 0.4621609 0.5378391 -0.210134060
## 13    0.005     0.3296420     0.8210181 0.3947630 0.6052370 -0.065121078
## 14    0.005     0.2976825     0.8983524 0.3712925 0.6287075 -0.073610033
## 15    0.005     0.3115117     0.4029339 0.4801525 0.5198475 -0.168640757
## 16    0.005     0.3247817     0.7219379 0.4151007 0.5848993 -0.090319052
## 17    0.005     0.3266347     0.8069795 0.3971877 0.6028123 -0.070552978
## 18    0.005     0.3173407     0.4336763 0.4748261 0.5251739 -0.157485349
## 19    0.005     0.3435630     1.0866387 0.3373388 0.6626612  0.006224251
## 20    0.005     0.2932514     0.5624413 0.4421955 0.5578045 -0.148944031
## 21    0.005     0.2798482     0.2346091 0.5100740 0.4899260 -0.230225830
## 22    0.005     0.3898919     1.0139979 0.3635290 0.6364710  0.026362956
## 23    0.005     0.3042464     0.9271827 0.3662506 0.6337494 -0.062004166
## 24    0.005     0.3630457     0.9002142 0.3841660 0.6158340 -0.021120398
## 25    0.005     0.2533002     0.4326950 0.4609611 0.5390389 -0.207660900
## 26    0.005     0.2460967     0.3932865 0.4677988 0.5322012 -0.221702109
## 27    0.005     0.3017434     0.9162639 0.3681653 0.6318347 -0.066421893
## 28    0.005     0.2789997     0.5700822 0.4374879 0.5625121 -0.158488237
## 29    0.005     0.3349306     1.0535536 0.3435102 0.6564898 -0.008579583
## 30    0.005     0.3244766     0.7968332 0.3989357 0.6010643 -0.074459095
## 31    0.005     0.3273243     1.0235111 0.3490268 0.6509732 -0.021702516
## 32    0.005     0.2772509     0.2207792 0.5126090 0.4873910 -0.235358190
## 33    0.005     0.2899054     0.5449504 0.4451686 0.5548314 -0.155263243
## 34    0.005     0.3008065     0.6015646 0.4355548 0.5644452 -0.134748318
## 35    0.005     0.3351455     1.0543902 0.3433554 0.6566446 -0.008209921
## 36    0.005     0.2902317     0.6282321 0.4276386 0.5723614 -0.137406949
## 37    0.005     0.3007245     0.3457603 0.4901524 0.5098476 -0.189427873
## 38    0.005     0.3403942     0.7968657 0.4022433 0.5977567 -0.061849133
## 39    0.005     0.3334453     0.7638745 0.4079237 0.5920763 -0.074478435
## 40    0.005     0.3381008     1.0658280 0.3412329 0.6587671 -0.003132154
## 41    0.005     0.2681036     0.5125103 0.4472729 0.5527271 -0.179169364
## 42    0.005     0.2823492     0.8285419 0.3833392 0.6166608 -0.100990021
## 43    0.005     0.3014529     0.3496299 0.4894711 0.5105289 -0.188018234
## 44    0.005     0.2910912     0.2945043 0.4992479 0.5007521 -0.208156703
## 45    0.005     0.2855844     0.8435571 0.3807649 0.6192351 -0.095180537
## 46    0.005     0.3370500     1.0617755 0.3419863 0.6580137 -0.004936392
## 47    0.005     0.3230483     0.7134434 0.4165498 0.5834502 -0.093501422
## 48    0.005     0.2752754     0.2102658 0.5145459 0.4854541 -0.239270443
## 49    0.005     0.3063638     0.6299972 0.4307320 0.5692680 -0.124368224
## 50    0.005     0.2727773     0.7832318 0.3910628 0.6089372 -0.118285542
## 51    0.005     0.2802328     0.2366572 0.5096998 0.4903002 -0.229467056
## 52    0.005     0.3213989     0.7822605 0.4014404 0.5985596 -0.080041503
## 53    0.005     0.3819771     0.9814543 0.3695306 0.6304694  0.012446546
## 54    0.005     0.2714530     0.5303221 0.4442399 0.5557601 -0.172786844
## 55    0.005     0.3569731     0.8731532 0.3889491 0.6110509 -0.031976010
## 56    0.005     0.3256477     1.0167758 0.3502531 0.6497469 -0.024605447
## 57    0.005     0.3341890     0.8420216 0.3911211 0.6088789 -0.056932157
## 58    0.005     0.3366717     0.5345042 0.4575228 0.5424772 -0.120851101
## 59    0.005     0.2964626     0.5791344 0.4393608 0.5606392 -0.142898163
## 60    0.005     0.3425530     1.0828227 0.3380561 0.6619439  0.004496965
## 61    0.005     0.2749861     0.7938043 0.3892660 0.6107340 -0.114279907
## 62    0.005     0.3185348     0.9877455 0.3554979 0.6445021 -0.036963127
## 63    0.005     0.3699515     1.1812766 0.3190225 0.6809775  0.050929006
## 64    0.005     0.3222009     0.4592041 0.4704246 0.5295754 -0.148223667
## 65    0.005     0.3893295     1.0117131 0.3639533 0.6360467  0.025376227
## 66    0.005     0.2667665     0.5053728 0.4484901 0.5515099 -0.181723642
## 67    0.005     0.2794543     0.5724606 0.4370846 0.5629154 -0.157630307
## 68    0.005     0.3116419     0.7352798 0.4094743 0.5905257 -0.097832388
## 69    0.005     0.3150037     0.7516000 0.4066898 0.5933102 -0.091686056
## 70    0.005     0.3097856     0.7262095 0.4110194 0.5889806 -0.101233771
## 71    0.005     0.3058270     0.7067290 0.4143327 0.5856673 -0.108505693
## 72    0.005     0.3868609     1.0016347 0.3658195 0.6341805  0.021041400
## 73    0.005     0.2854063     0.2642180 0.5046928 0.4953072 -0.219286524
## 74    0.005     0.2565419     0.4503158 0.4579227 0.5420773 -0.201380772
## 75    0.005     0.2890843     0.8596295 0.3779999 0.6220001 -0.088915569
## 76    0.005     0.3245694     1.0124225 0.3510438 0.6489562 -0.026474398
## 77    0.005     0.3140113     0.9688974 0.3588701 0.6411299 -0.044858713
## 78    0.005     0.2694677     0.7672608 0.3937718 0.6062282 -0.124304155
## 79    0.005     0.2842256     0.8372693 0.3818439 0.6181561 -0.097618310
## 80    0.005     0.2740094     0.2035310 0.5157911 0.4842089 -0.241781687
## 81    0.005     0.3026634     0.3560580 0.4883410 0.5116590 -0.185677642
## 82    0.005     0.3149827     0.9729703 0.3581435 0.6418565 -0.043160735
## 83    0.005     0.3406182     1.0754723 0.3394336 0.6605664  0.001184645
## 84    0.005     0.3631420     0.9006397 0.3840905 0.6159095 -0.020948484
## 85    0.005     0.2424349     0.3731310 0.4713218 0.5286782 -0.228886906
## 86    0.005     0.3341705     0.5215742 0.4597318 0.5402682 -0.125561344
## 87    0.005     0.3311291     0.7527492 0.4098320 0.5901680 -0.078702813
## 88    0.005     0.3691809     0.9270638 0.3793782 0.6206218 -0.010197248
## 89    0.005     0.3721752     1.1888157 0.3175146 0.6824854  0.054660627
## 90    0.005     0.3285232     0.4922439 0.4647513 0.5352487 -0.136228083
## 91    0.005     0.3542347     1.1260751 0.3298352 0.6701648  0.024399455
## 92    0.005     0.3730797     1.1918634 0.3169027 0.6830973  0.056176982
## 93    0.005     0.3194352     0.9914613 0.3548302 0.6451698 -0.035394953
## 94    0.005     0.2590474     0.4638820 0.4555905 0.5444095 -0.196543120
## 95    0.005     0.2932211     0.5622830 0.4422224 0.5577776 -0.149001288
## 96    0.005     0.3456327     1.0944127 0.3358729 0.6641271  0.009759827
## 97    0.005     0.3656378     0.9116185 0.3821378 0.6178622 -0.016500032
## 98    0.005     0.3817010     0.9803037 0.3697412 0.6302588  0.011959807
## 99    0.005     0.2518155     0.4246004 0.4623605 0.5376395 -0.210545058
## 100   0.005     0.3227496     0.7119762 0.4167999 0.5832001 -0.094050235
## 101   0.005     0.2908963     0.5501402 0.4442861 0.5557139 -0.153389805
## 102   0.005     0.3335630     0.5184279 0.4602697 0.5397303 -0.126706691
## 103   0.005     0.2649895     0.1556330 0.5247580 0.4752420 -0.259768463
## 104   0.005     0.3445467     1.0903409 0.3366415 0.6633585  0.007905186
## 105   0.005     0.2657598     0.4999893 0.4494090 0.5505910 -0.183649135
## 106   0.005     0.2890959     0.5407047 0.4458909 0.5541091 -0.156794968
## 107   0.005     0.3197241     0.4462075 0.4726633 0.5273367 -0.152939172
## 108   0.005     0.2845541     0.2596778 0.5055141 0.4944859 -0.220959931
## 109   0.005     0.3243335     0.7197446 0.4154750 0.5845250 -0.091141551
## 110   0.005     0.3409044     0.5562930 0.4538041 0.5461959 -0.112899765
## 111   0.005     0.2547050     0.4403400 0.4596415 0.5403585 -0.204936536
## 112   0.005     0.2910158     0.8684225 0.3764827 0.6235173 -0.085466933
## 113   0.005     0.2508579     0.4193720 0.4632658 0.5367342 -0.212407870
## 114   0.005     0.2964258     0.8927608 0.3722654 0.6277346 -0.075839539
## 115   0.005     0.3588522     0.8815781 0.3874643 0.6125357 -0.028612120
## 116   0.005     0.3827203     0.9845458 0.3689642 0.6310358  0.013756062
## 117   0.005     0.3607122     0.8898724 0.3859988 0.6140012 -0.025286572
## 118   0.005     0.2966595     0.6609150 0.4221043 0.5778957 -0.125444778
## 119   0.005     0.2624694     0.4823341 0.4524272 0.5475728 -0.189957806
## 120   0.005     0.3669508     0.9173617 0.3811134 0.6188866 -0.014162601
## 121   0.005     0.3274447     1.0239933 0.3489389 0.6510611 -0.021494154
## 122   0.005     0.3206376     0.4510041 0.4718365 0.5281635 -0.151198938
## 123   0.005     0.3315804     0.7549220 0.4094595 0.5905405 -0.077879090
## 124   0.005     0.3612279     1.1510550 0.3249901 0.6750099  0.036237827
## 125   0.005     0.3452590     0.8196100 0.3983055 0.6016945 -0.053046496
## 126   0.005     0.3140272     0.9689641 0.3588582 0.6411418 -0.044830937
## 127   0.005     0.2822477     0.5870285 0.4346157 0.5653843 -0.152367952
## 128   0.005     0.3056444     0.6263339 0.4313534 0.5686466 -0.125709002
## 129   0.005     0.2973471     0.8968625 0.3715519 0.6284481 -0.074204780
## 130   0.005     0.3332153     0.7627725 0.4081129 0.5918871 -0.074897617
## 131   0.005     0.3436371     0.8120599 0.3996149 0.6003851 -0.055977727
## 132   0.005     0.3464715     0.8252327 0.3973289 0.6026711 -0.050857449
## 133   0.005     0.3221389     1.0025477 0.3528317 0.6471683 -0.030692822
## 134   0.005     0.3020708     0.6080598 0.4344530 0.5655470 -0.132382205
## 135   0.005     0.2998152     0.6767935 0.4194133 0.5805867 -0.119598134
## 136   0.005     0.2929347     0.8771042 0.3749815 0.6250185 -0.082046777
## 137   0.005     0.3365846     0.7788514 0.4053493 0.5946507 -0.068764651
## 138   0.005     0.2728169     0.5375466 0.4430113 0.5569887 -0.170194394
## 139   0.005     0.3092091     0.7233839 0.4115004 0.5884996 -0.102291329
## 140   0.005     0.3532030     0.8561119 0.3919415 0.6080585 -0.038738427
## 141   0.005     0.3483369     0.5942540 0.4473313 0.5526687 -0.098994443
## 142   0.005     0.2940299     0.6475992 0.4243596 0.5756404 -0.130329680
## 143   0.005     0.2566568     0.4509390 0.4578154 0.5421846 -0.201158584
## 144   0.005     0.3717160     1.1872642 0.3178256 0.6821744  0.053890408
## 145   0.005     0.3173822     0.7630627 0.4047302 0.5952698 -0.087347987
## 146   0.005     0.2420791     0.3711684 0.4716659 0.5283341 -0.229586798
## 147   0.005     0.2932364     0.5623629 0.4422088 0.5577912 -0.148972410
## 148   0.005     0.3051386     0.6237548 0.4317909 0.5682091 -0.126652292
## 149   0.005     0.3410795     1.0772294 0.3391048 0.6608952  0.001974703
## 150   0.005     0.3344677     0.5231124 0.4594689 0.5405311 -0.125001280
## 151   0.005     0.2928397     0.5602944 0.4425602 0.5574398 -0.149720490
## 152   0.005     0.2982554     0.6689591 0.4207413 0.5792587 -0.122485906
## 153   0.005     0.2403793     0.3617833 0.4733139 0.5266861 -0.232934554
## 154   0.005     0.2417641     0.3694304 0.4719707 0.5280293 -0.230206641
## 155   0.005     0.3163082     0.7578956 0.4056139 0.5943861 -0.089305683
## 156   0.005     0.2547211     0.4404276 0.4596264 0.5403736 -0.204905303
## 157   0.005     0.2828385     0.8308227 0.3829487 0.6170513 -0.100110206
## 158   0.005     0.2737568     0.2021870 0.5160401 0.4839599 -0.242283327
## 159   0.005     0.2951144     0.5721371 0.4405487 0.5594513 -0.145434303
## 160   0.005     0.2828816     0.2507672 0.5071299 0.4928701 -0.224248288
## 161   0.005     0.2536844     0.4347874 0.4605997 0.5394003 -0.206915305
## 162   0.005     0.2929317     0.8770906 0.3749838 0.6250162 -0.082052143
## 163   0.005     0.3252684     0.8005625 0.3982936 0.6017064 -0.073025261
## 164   0.005     0.2681058     0.1721612 0.5216410 0.4783590 -0.253535206
## 165   0.005     0.3169572     0.6833261 0.4216773 0.5783227 -0.104720105
## 166   0.005     0.3159522     0.9770212 0.3574197 0.6425803 -0.041467493
## 167   0.005     0.3159158     0.6781362 0.4225596 0.5774404 -0.106643808
## 168   0.005     0.2905677     0.2917160 0.4997469 0.5002531 -0.209179125
## 169   0.005     0.3220142     0.7851837 0.4009385 0.5990615 -0.078924281
## 170   0.005     0.3008845     0.6019657 0.4354867 0.5645133 -0.134602266
## 171   0.005     0.3527100     0.6163963 0.4435559 0.5564441 -0.090845950
## 172   0.005     0.3222769     0.4596021 0.4703561 0.5296439 -0.148079253
## 173   0.005     0.2900104     0.6270991 0.4278305 0.5721695 -0.137820047
## 174   0.005     0.2954439     0.5738484 0.4402582 0.5597418 -0.144814312
## 175   0.005     0.2762491     0.5556521 0.4399359 0.5600641 -0.163686810
## 176   0.005     0.2681418     0.5127139 0.4472382 0.5527618 -0.179096463
## 177   0.005     0.3225109     0.7875398 0.4005338 0.5994662 -0.078022878
## 178   0.005     0.2433088     0.3779480 0.4704781 0.5295219 -0.227169328
## 179   0.005     0.2959276     0.6572166 0.4227308 0.5772692 -0.126803167
## 180   0.005     0.3498035     0.6016965 0.4460625 0.5539375 -0.096258989
## 181   0.005     0.3015434     0.3501106 0.4893866 0.5106134 -0.187843163
## 182   0.005     0.2728893     0.5379297 0.4429462 0.5570538 -0.170056848
## 183   0.005     0.3037479     0.3618147 0.4873305 0.5126695 -0.183582574
## 184   0.005     0.2947134     0.3137908 0.4958090 0.5041910 -0.201095640
## 185   0.005     0.2649461     0.4956316 0.4501532 0.5498468 -0.185207063
## 186   0.005     0.3445565     0.5749959 0.4506145 0.5493855 -0.106058012
## 187   0.005     0.3092544     0.7236063 0.4114626 0.5885374 -0.102208118
## 188   0.005     0.3864202     0.9998269 0.3661533 0.6338467  0.020266888
## 189   0.005     0.3521790     0.6137159 0.4440130 0.5559870 -0.091834032
## 190   0.005     0.2474832     0.4008974 0.4664733 0.5335267 -0.218990076
## 191   0.005     0.3080119     0.6383703 0.4293115 0.5706885 -0.121299679
## 192   0.005     0.2991539     0.5930520 0.4369990 0.5630010 -0.137845054
## 193   0.005     0.2887617     0.6206955 0.4289145 0.5710855 -0.140152813
## 194   0.005     0.3258409     1.0175540 0.3501116 0.6498884 -0.024270733
## 195   0.005     0.2759310     0.5539784 0.4402200 0.5597800 -0.164289059
## 196   0.005     0.3143876     0.6704989 0.4238574 0.5761426 -0.109469819
## 197   0.005     0.3261343     0.7285429 0.4139731 0.5860269 -0.087838735
## 198   0.005     0.3293195     0.7440125 0.4113282 0.5886718 -0.082008749
## 199   0.005     0.2678881     0.1710058 0.5218581 0.4781419 -0.253969980
## 200   0.005     0.3866762     1.0008775 0.3659593 0.6340407  0.020716894
##         e_dual_0
## 1    0.397429497
## 2    0.367143095
## 3   -0.196842306
## 4    0.241979258
## 5   -0.069551698
## 6    0.024787504
## 7    0.031919661
## 8    0.418684955
## 9    0.254907075
## 10   0.074144880
## 11   0.386262279
## 12  -0.112085101
## 13   0.215781130
## 14   0.269644925
## 15  -0.116913558
## 16   0.137038663
## 17   0.204167196
## 18  -0.091497639
## 19   0.423977457
## 20   0.004636775
## 21  -0.255316845
## 22   0.377526841
## 23   0.293433246
## 24   0.284380213
## 25  -0.106343961
## 26  -0.138914708
## 27   0.284429278
## 28   0.007570132
## 29   0.397063780
## 30   0.195768936
## 31   0.372537937
## 32  -0.266611745
## 33  -0.009880966
## 34   0.037119371
## 35   0.397745614
## 36   0.055870780
## 37  -0.164087293
## 38   0.199109080
## 39   0.171798208
## 40   0.407060922
## 41  -0.040216811
## 42   0.211881123
## 43  -0.160898971
## 44  -0.206247772
## 45   0.224322042
## 46   0.403761800
## 47   0.129993105
## 48  -0.275188359
## 49   0.060729190
## 50   0.174294650
## 51  -0.253642944
## 52   0.183700885
## 53   0.350984919
## 54  -0.025438054
## 55   0.262102329
## 56   0.367028919
## 57   0.233142693
## 58  -0.007972911
## 59   0.018495160
## 60   0.420878805
## 61   0.183070239
## 62   0.343243412
## 63   0.500299116
## 64  -0.070371271
## 65   0.375666397
## 66  -0.046137042
## 67   0.009545222
## 68   0.144754107
## 69   0.158289721
## 70   0.137228914
## 71   0.121061699
## 72   0.367454148
## 73  -0.231089173
## 74  -0.091761523
## 75   0.237629401
## 76   0.363466245
## 77   0.327767475
## 78   0.161032621
## 79   0.219113270
## 80  -0.280677898
## 81  -0.155601051
## 82   0.331113753
## 83   0.414905856
## 84   0.284730226
## 85  -0.155547204
## 86  -0.018693929
## 87   0.162581147
## 88   0.306441975
## 89   0.506330324
## 90  -0.043004816
## 91   0.455910363
## 92   0.508766149
## 93   0.346291459
## 94  -0.080527509
## 95   0.004505376
## 96   0.430285621
## 97   0.293756332
## 98   0.350044900
## 99  -0.113039055
## 100  0.128776076
## 101 -0.005573751
## 102 -0.021302406
## 103 -0.319608969
## 104  0.426982355
## 105 -0.050601727
## 106 -0.013404397
## 107 -0.081129199
## 108 -0.234808105
## 109  0.135219611
## 110  0.010097117
## 111 -0.100018488
## 112  0.244905226
## 113 -0.117362249
## 114  0.265026163
## 115  0.269042408
## 116  0.353510048
## 117  0.275871163
## 118  0.083019313
## 119 -0.065238750
## 120  0.298475087
## 121  0.372932196
## 122 -0.077159313
## 123  0.164381508
## 124  0.476045135
## 125  0.217915468
## 126  0.327822295
## 127  0.021644176
## 128  0.057687256
## 129  0.268414337
## 130  0.170885385
## 131  0.211674753
## 132  0.222561607
## 133  0.355379407
## 134  0.042512787
## 135  0.096206774
## 136  0.252085684
## 137  0.184200726
## 138 -0.019442117
## 139  0.134884304
## 140  0.248053384
## 141  0.041585300
## 142  0.071958783
## 143 -0.091245551
## 144  0.505089755
## 145  0.167792934
## 146 -0.157165736
## 147  0.004571647
## 148  0.055545708
## 149  0.416334206
## 150 -0.017418613
## 151  0.002854641
## 152  0.089700345
## 153 -0.164902845
## 154 -0.158598875
## 155  0.163509550
## 156 -0.099945966
## 157  0.213771371
## 158 -0.281772967
## 159  0.012685847
## 160 -0.242102937
## 161 -0.104612934
## 162  0.252074447
## 163  0.198856180
## 164 -0.306197819
## 165  0.105003432
## 166  0.334440845
## 167  0.100695797
## 168 -0.208537114
## 169  0.186122180
## 170  0.037452476
## 171  0.059952254
## 172 -0.070041777
## 173  0.054929534
## 174  0.014106564
## 175 -0.004411991
## 176 -0.040047887
## 177  0.188073539
## 178 -0.151573869
## 179  0.079947368
## 180  0.047758936
## 181 -0.160502866
## 182 -0.019124116
## 183 -0.150854868
## 184 -0.190400184
## 185 -0.054215166
## 186  0.025610447
## 187  0.135068865
## 188  0.365980173
## 189  0.057728935
## 190 -0.132629356
## 191  0.067681842
## 192  0.030050947
## 193  0.049609999
## 194  0.367665653
## 195 -0.005801599
## 196  0.094356350
## 197  0.142515982
## 198  0.155340741
## 199 -0.307136112
## 200  0.366836825
## 
## $values
## $values$divergencekl
## [1] -18.65884
## 
## $values$iterations
## [1] 70
## 
## $values$message
## [1] &quot;relative convergence (4)&quot;
## 
## 
## $tol
## [1] 1e-10
## 
## $v
##      [,1] [,2] [,3]
## [1,]    1    0   -1
## 
## $lambda
##   (Intercept) Dcollege   Totalincome     Dunemp
## 1   -15.98441 33.92671 -9.263394e-05  -9.876409
## 0    73.91737 98.95686 -2.072908e-04 -52.600547
## 
## $checkrestrictions
## $checkrestrictions$g1
##        [,1]
##   [1,]    0
##   [2,]    0
##   [3,]    0
##   [4,]    0
##   [5,]    0
##   [6,]    0
##   [7,]    0
##   [8,]    0
##   [9,]    0
##  [10,]    0
##  [11,]    0
##  [12,]    0
##  [13,]    0
##  [14,]    0
##  [15,]    0
##  [16,]    0
##  [17,]    0
##  [18,]    0
##  [19,]    0
##  [20,]    0
##  [21,]    0
##  [22,]    0
##  [23,]    0
##  [24,]    0
##  [25,]    0
##  [26,]    0
##  [27,]    0
##  [28,]    0
##  [29,]    0
##  [30,]    0
##  [31,]    0
##  [32,]    0
##  [33,]    0
##  [34,]    0
##  [35,]    0
##  [36,]    0
##  [37,]    0
##  [38,]    0
##  [39,]    0
##  [40,]    0
##  [41,]    0
##  [42,]    0
##  [43,]    0
##  [44,]    0
##  [45,]    0
##  [46,]    0
##  [47,]    0
##  [48,]    0
##  [49,]    0
##  [50,]    0
##  [51,]    0
##  [52,]    0
##  [53,]    0
##  [54,]    0
##  [55,]    0
##  [56,]    0
##  [57,]    0
##  [58,]    0
##  [59,]    0
##  [60,]    0
##  [61,]    0
##  [62,]    0
##  [63,]    0
##  [64,]    0
##  [65,]    0
##  [66,]    0
##  [67,]    0
##  [68,]    0
##  [69,]    0
##  [70,]    0
##  [71,]    0
##  [72,]    0
##  [73,]    0
##  [74,]    0
##  [75,]    0
##  [76,]    0
##  [77,]    0
##  [78,]    0
##  [79,]    0
##  [80,]    0
##  [81,]    0
##  [82,]    0
##  [83,]    0
##  [84,]    0
##  [85,]    0
##  [86,]    0
##  [87,]    0
##  [88,]    0
##  [89,]    0
##  [90,]    0
##  [91,]    0
##  [92,]    0
##  [93,]    0
##  [94,]    0
##  [95,]    0
##  [96,]    0
##  [97,]    0
##  [98,]    0
##  [99,]    0
## [100,]    0
## [101,]    0
## [102,]    0
## [103,]    0
## [104,]    0
## [105,]    0
## [106,]    0
## [107,]    0
## [108,]    0
## [109,]    0
## [110,]    0
## [111,]    0
## [112,]    0
## [113,]    0
## [114,]    0
## [115,]    0
## [116,]    0
## [117,]    0
## [118,]    0
## [119,]    0
## [120,]    0
## [121,]    0
## [122,]    0
## [123,]    0
## [124,]    0
## [125,]    0
## [126,]    0
## [127,]    0
## [128,]    0
## [129,]    0
## [130,]    0
## [131,]    0
## [132,]    0
## [133,]    0
## [134,]    0
## [135,]    0
## [136,]    0
## [137,]    0
## [138,]    0
## [139,]    0
## [140,]    0
## [141,]    0
## [142,]    0
## [143,]    0
## [144,]    0
## [145,]    0
## [146,]    0
## [147,]    0
## [148,]    0
## [149,]    0
## [150,]    0
## [151,]    0
## [152,]    0
## [153,]    0
## [154,]    0
## [155,]    0
## [156,]    0
## [157,]    0
## [158,]    0
## [159,]    0
## [160,]    0
## [161,]    0
## [162,]    0
## [163,]    0
## [164,]    0
## [165,]    0
## [166,]    0
## [167,]    0
## [168,]    0
## [169,]    0
## [170,]    0
## [171,]    0
## [172,]    0
## [173,]    0
## [174,]    0
## [175,]    0
## [176,]    0
## [177,]    0
## [178,]    0
## [179,]    0
## [180,]    0
## [181,]    0
## [182,]    0
## [183,]    0
## [184,]    0
## [185,]    0
## [186,]    0
## [187,]    0
## [188,]    0
## [189,]    0
## [190,]    0
## [191,]    0
## [192,]    0
## [193,]    0
## [194,]    0
## [195,]    0
## [196,]    0
## [197,]    0
## [198,]    0
## [199,]    0
## [200,]    0
## 
## $checkrestrictions$g2
##        [,1] [,2]
##   [1,]    0    0
##   [2,]    0    0
##   [3,]    0    0
##   [4,]    0    0
##   [5,]    0    0
##   [6,]    0    0
##   [7,]    0    0
##   [8,]    0    0
##   [9,]    0    0
##  [10,]    0    0
##  [11,]    0    0
##  [12,]    0    0
##  [13,]    0    0
##  [14,]    0    0
##  [15,]    0    0
##  [16,]    0    0
##  [17,]    0    0
##  [18,]    0    0
##  [19,]    0    0
##  [20,]    0    0
##  [21,]    0    0
##  [22,]    0    0
##  [23,]    0    0
##  [24,]    0    0
##  [25,]    0    0
##  [26,]    0    0
##  [27,]    0    0
##  [28,]    0    0
##  [29,]    0    0
##  [30,]    0    0
##  [31,]    0    0
##  [32,]    0    0
##  [33,]    0    0
##  [34,]    0    0
##  [35,]    0    0
##  [36,]    0    0
##  [37,]    0    0
##  [38,]    0    0
##  [39,]    0    0
##  [40,]    0    0
##  [41,]    0    0
##  [42,]    0    0
##  [43,]    0    0
##  [44,]    0    0
##  [45,]    0    0
##  [46,]    0    0
##  [47,]    0    0
##  [48,]    0    0
##  [49,]    0    0
##  [50,]    0    0
##  [51,]    0    0
##  [52,]    0    0
##  [53,]    0    0
##  [54,]    0    0
##  [55,]    0    0
##  [56,]    0    0
##  [57,]    0    0
##  [58,]    0    0
##  [59,]    0    0
##  [60,]    0    0
##  [61,]    0    0
##  [62,]    0    0
##  [63,]    0    0
##  [64,]    0    0
##  [65,]    0    0
##  [66,]    0    0
##  [67,]    0    0
##  [68,]    0    0
##  [69,]    0    0
##  [70,]    0    0
##  [71,]    0    0
##  [72,]    0    0
##  [73,]    0    0
##  [74,]    0    0
##  [75,]    0    0
##  [76,]    0    0
##  [77,]    0    0
##  [78,]    0    0
##  [79,]    0    0
##  [80,]    0    0
##  [81,]    0    0
##  [82,]    0    0
##  [83,]    0    0
##  [84,]    0    0
##  [85,]    0    0
##  [86,]    0    0
##  [87,]    0    0
##  [88,]    0    0
##  [89,]    0    0
##  [90,]    0    0
##  [91,]    0    0
##  [92,]    0    0
##  [93,]    0    0
##  [94,]    0    0
##  [95,]    0    0
##  [96,]    0    0
##  [97,]    0    0
##  [98,]    0    0
##  [99,]    0    0
## [100,]    0    0
## [101,]    0    0
## [102,]    0    0
## [103,]    0    0
## [104,]    0    0
## [105,]    0    0
## [106,]    0    0
## [107,]    0    0
## [108,]    0    0
## [109,]    0    0
## [110,]    0    0
## [111,]    0    0
## [112,]    0    0
## [113,]    0    0
## [114,]    0    0
## [115,]    0    0
## [116,]    0    0
## [117,]    0    0
## [118,]    0    0
## [119,]    0    0
## [120,]    0    0
## [121,]    0    0
## [122,]    0    0
## [123,]    0    0
## [124,]    0    0
## [125,]    0    0
## [126,]    0    0
## [127,]    0    0
## [128,]    0    0
## [129,]    0    0
## [130,]    0    0
## [131,]    0    0
## [132,]    0    0
## [133,]    0    0
## [134,]    0    0
## [135,]    0    0
## [136,]    0    0
## [137,]    0    0
## [138,]    0    0
## [139,]    0    0
## [140,]    0    0
## [141,]    0    0
## [142,]    0    0
## [143,]    0    0
## [144,]    0    0
## [145,]    0    0
## [146,]    0    0
## [147,]    0    0
## [148,]    0    0
## [149,]    0    0
## [150,]    0    0
## [151,]    0    0
## [152,]    0    0
## [153,]    0    0
## [154,]    0    0
## [155,]    0    0
## [156,]    0    0
## [157,]    0    0
## [158,]    0    0
## [159,]    0    0
## [160,]    0    0
## [161,]    0    0
## [162,]    0    0
## [163,]    0    0
## [164,]    0    0
## [165,]    0    0
## [166,]    0    0
## [167,]    0    0
## [168,]    0    0
## [169,]    0    0
## [170,]    0    0
## [171,]    0    0
## [172,]    0    0
## [173,]    0    0
## [174,]    0    0
## [175,]    0    0
## [176,]    0    0
## [177,]    0    0
## [178,]    0    0
## [179,]    0    0
## [180,]    0    0
## [181,]    0    0
## [182,]    0    0
## [183,]    0    0
## [184,]    0    0
## [185,]    0    0
## [186,]    0    0
## [187,]    0    0
## [188,]    0    0
## [189,]    0    0
## [190,]    0    0
## [191,]    0    0
## [192,]    0    0
## [193,]    0    0
## [194,]    0    0
## [195,]    0    0
## [196,]    0    0
## [197,]    0    0
## [198,]    0    0
## [199,]    0    0
## [200,]    0    0
## 
## $checkrestrictions$g3
##             y_factor1 y_factor0
## (Intercept)         0         0
## Dcollege            0         0
## Totalincome         0         0
## Dunemp              0         0
## 
## 
## $cross_moments_hp
##                    1         0
## (Intercept)     0.31      0.69
## Dcollege        0.17      0.45
## Totalincome 94640.65 197483.48
## Dunemp          0.15      0.28
## 
## $cross_moments_hs
##             1          0         
## (Intercept)       0.31       0.69
## Dcollege          0.17       0.45
## Totalincome   94640.65  197483.48
## Dunemp            0.15       0.28
## 
## $J
## [1] 2
## 
## $fn
## datahp$poor_liq ~ Dcollege + Totalincome + Dunemp
## 
## attr(,&quot;class&quot;)
## [1] &quot;kl&quot;</code></pre>
<p>The function will produce a data frame called a table with the
following information:</p>
<ul>
<li><p>Probabilities for each individual to each possibility <span class="math inline">\(j\)</span> of the variable of interest <span class="math inline">\(Y\)</span>.</p></li>
<li><p>The errors are calculated to the <span class="math inline">\(j\)</span> possibilities of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>The prediction for each individual is the result of the sum of
the probability plus the error.</p></li>
</ul>
<p>The function provides information about the optimization process, in
concrete:</p>
<ul>
<li>The value of entropy resulting from the optimization, the
iterations, which indicate the times the objective function and the
gradient have been evaluated during the optimization process and a
message in case something went wrong.</li>
</ul>
<p>Lagrange multipliers <span class="math inline">\(\lambda\)</span>
associated to each independent variables are also provided in the form
of a data frame. In addition, it provides an object with the
restrictions checked which should be approximately zero. Being g1 the
restriction related to the unit probability constraint, g2 to the error
unit sum constraint, and g3 to the consistency restriction that implies
that the difference between the cross moment in both datasets must be
zero. The restriction g3 can be checked thoroughly with the objects
separately which are to be provided in the output as cross moments hp
(the cross moments in datahp) and cross moments hs (the cross moments in
datahs).</p>
<p>To make the results more visual, this package includes a personalized
summary function, providing the means for each category <span class="math inline">\(j\)</span> for the predictions, the probabilities
and the error.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<pre><code>## 
## Adjuntando el paquete: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">summary</span>(result)</span></code></pre></div>
<pre><code>## Iterations[1] 70
## Kullback-Leibler divergence value[1] -18.65884
## [1] &quot;mean_estimations&quot;
##   weights predictions_1 predictions_0 p_dual_1 p_dual_0  e_dual_1 e_dual_0
## 1   0.005          0.31          0.69 0.417955 0.582045 -0.107955 0.107955
## [1] &quot;lambda&quot;
##   (Intercept) Dcollege   Totalincome     Dunemp
## 1   -15.98441 33.92671 -9.263394e-05  -9.876409
## 0    73.91737 98.95686 -2.072908e-04 -52.600547</code></pre>
<p>Graphs included are generated with the plot function, showing the
averages of the predictions for each territorial unit and the 95%
confidence interval associated with each of them.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span>result,datahs<span class="sc">$</span>reg)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --><img role="img" src="data:image/png;base64,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" /><!-- --></p>
<p>Notes: Arguments <em>tol and iter</em> can be defined by the user.
The default tolerance has been set in 1e-10 while the maximum number or
iterations by default is settled in 1000.</p>
</div>
<div id="without-prior-information" class="section level1">
<h1>3.2 Without prior information</h1>
<p>Suppose we do not have prior information about our variable of
interest. In that case, the process starting point will be the uniform
distribution as we do not have information to think one characteristic
is more likely than the other.</p>
<p>The function ei_gce() will include by default the uniform
distribution as <span class="math inline">\(Q\)</span> if the user does
not specify any other. This would be:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>result2 <span class="ot">&lt;-</span> <span class="fu">ei_gce</span>(fn,datahp,datahs,<span class="at">weights=</span>w)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>result2</span></code></pre></div>
<pre><code>## $estimations
##     weights predictions_1 predictions_0  p_dual_1  p_dual_0     e_dual_1
## 1     0.005     0.3482709     1.0365048 0.1603119 0.8396881  0.187959050
## 2     0.005     0.3857589     0.9968176 0.1983963 0.8016037  0.187362606
## 3     0.005     0.3173375     0.2993646 0.5088871 0.4911129 -0.191549583
## 4     0.005     0.3158820     0.9034151 0.2202346 0.7797654  0.095647393
## 5     0.005     0.2329586     0.4517758 0.3934611 0.6065389 -0.160502559
## 6     0.005     0.2476947     0.5746948 0.3446132 0.6553868 -0.096918569
## 7     0.005     0.2506221     0.5857495 0.3410379 0.6589621 -0.090415733
## 8     0.005     0.3461909     1.0481479 0.1526572 0.8473428  0.193533734
## 9     0.005     0.3226843     0.9190089 0.2149782 0.7850218  0.107706173
## 10    0.005     0.2762950     0.6334311 0.3317820 0.6682180 -0.055487048
## 11    0.005     0.3489347     1.0299689 0.1643841 0.8356159  0.184550621
## 12    0.005     0.2389999     0.4091960 0.4164582 0.5835418 -0.177458282
## 13    0.005     0.3615030     0.8753045 0.2542609 0.7457391  0.107242115
## 14    0.005     0.3294738     0.9355820 0.2090479 0.7909521  0.120425958
## 15    0.005     0.2964828     0.3716954 0.4632388 0.5367612 -0.166755905
## 16    0.005     0.3175745     0.7408777 0.3011592 0.6988408  0.016415292
## 17    0.005     0.3560950     0.8606808 0.2594455 0.7405545  0.096649523
## 18    0.005     0.2942372     0.3990379 0.4490490 0.5509510 -0.154811740
## 19    0.005     0.3455253     1.0509007 0.1507707 0.8492293  0.194754559
## 20    0.005     0.2471631     0.5249938 0.3669615 0.6330385 -0.119798402
## 21    0.005     0.3417450     0.2546507 0.5432679 0.4567321 -0.201522838
## 22    0.005     0.3842934     1.0032907 0.1940898 0.8059102  0.190203555
## 23    0.005     0.3382043     0.9597581 0.1996137 0.8003863  0.138590550
## 24    0.005     0.3861360     0.9337598 0.2337266 0.7662734  0.152409398
## 25    0.005     0.2378185     0.4145818 0.4133178 0.5866822 -0.175499305
## 26    0.005     0.2458231     0.3852597 0.4312859 0.5687141 -0.185462778
## 27    0.005     0.3352231     0.9509670 0.2031647 0.7968353  0.132058395
## 28    0.005     0.2416926     0.5489804 0.3533114 0.6466886 -0.111618803
## 29    0.005     0.3482976     1.0362956 0.1604447 0.8395553  0.187852975
## 30    0.005     0.3518132     0.8496147 0.2632188 0.7367812  0.088594371
## 31    0.005     0.3492859     1.0214876 0.1694369 0.8305631  0.179849021
## 32    0.005     0.3470708     0.2464818 0.5500045 0.4499955 -0.202933677
## 33    0.005     0.2441365     0.5052141 0.3744907 0.6255093 -0.130354118
## 34    0.005     0.2578511     0.5730890 0.3503439 0.6496561 -0.092492809
## 35    0.005     0.3482475     1.0366854 0.1601971 0.8398029  0.188050445
## 36    0.005     0.2622390     0.6243671 0.3291491 0.6708509 -0.066910086
## 37    0.005     0.3066472     0.3269655 0.4899921 0.5100079 -0.183344942
## 38    0.005     0.3568758     0.8365689 0.2720294 0.7279706  0.084846417
## 39    0.005     0.3408160     0.7967538 0.2847148 0.7152852  0.056101241
## 40    0.005     0.3474526     1.0419027 0.1568277 0.8431723  0.190624886
## 41    0.005     0.2329565     0.4853631 0.3779574 0.6220426 -0.145000926
## 42    0.005     0.2972922     0.8632486 0.2326726 0.7673274  0.064619546
## 43    0.005     0.3057493     0.3297838 0.4881669 0.5118331 -0.182417532
## 44    0.005     0.3208500     0.2918338 0.5143610 0.4856390 -0.193511033
## 45    0.005     0.3054033     0.8804988 0.2274973 0.7725027  0.077906058
## 46    0.005     0.3477572     1.0400775 0.1580183 0.8419817  0.189738933
## 47    0.005     0.3127014     0.7290300 0.3045337 0.6954663  0.008167655
## 48    0.005     0.3512239     0.2403536 0.5551404 0.4448596 -0.203916512
## 49    0.005     0.2689096     0.6110202 0.3384614 0.6615386 -0.069551804
## 50    0.005     0.2702648     0.8061794 0.2486060 0.7513940  0.021658774
## 51    0.005     0.3409704     0.2558716 0.5422722 0.4577278 -0.201301771
## 52    0.005     0.3451508     0.8329972 0.2686767 0.7313233  0.076474117
## 53    0.005     0.3875208     0.9862174 0.2051533 0.7948467  0.182367472
## 54    0.005     0.2344435     0.5038116 0.3702553 0.6297447 -0.135811830
## 55    0.005     0.3817622     0.9121441 0.2435452 0.7564548  0.138216993
## 56    0.005     0.3492698     1.0179319 0.1714801 0.8285199  0.177789744
## 57    0.005     0.3684236     0.8957109 0.2465824 0.7534176  0.121841256
## 58    0.005     0.3074436     0.5101620 0.4035159 0.5964841 -0.096072316
## 59    0.005     0.2510605     0.5448791 0.3598334 0.6401666 -0.108772890
## 60    0.005     0.3459216     1.0492956 0.1518743 0.8481257  0.194047345
## 61    0.005     0.2768015     0.8200927 0.2448455 0.7551545  0.031956025
## 62    0.005     0.3480429     1.0014620 0.1804012 0.8195988  0.167641770
## 63    0.005     0.3312517     1.0856548 0.1244220 0.8755780  0.206829744
## 64    0.005     0.2944264     0.4238893 0.4373731 0.5626269 -0.142946701
## 65    0.005     0.3845737     1.0021492 0.1948593 0.8051407  0.189714387
## 66    0.005     0.2326499     0.4782694 0.3810631 0.6189369 -0.148413212
## 67    0.005     0.2423037     0.5518575 0.3523087 0.6476913 -0.110005071
## 68    0.005     0.3203930     0.7740449 0.2865812 0.7134188  0.033811842
## 69    0.005     0.3294487     0.7953810 0.2803080 0.7196920  0.049140708
## 70    0.005     0.3152276     0.7618539 0.2900924 0.7099076  0.025135265
## 71    0.005     0.3039823     0.7350242 0.2976932 0.7023068  0.006289109
## 72    0.005     0.3857186     0.9970152 0.1982669 0.8017331  0.187451719
## 73    0.005     0.3309330     0.2726195 0.5289221 0.4710779 -0.197989113
## 74    0.005     0.2353073     0.4287395 0.4053924 0.5946076 -0.170085078
## 75    0.005     0.3134230     0.8979430 0.2220148 0.7779852  0.091408166
## 76    0.005     0.3492085     1.0155834 0.1728060 0.8271940  0.176402479
## 77    0.005     0.3461210     0.9896499 0.1862930 0.8137070  0.159828056
## 78    0.005     0.2603061     0.7846246 0.2543364 0.7456636  0.005969729
## 79    0.005     0.3020743     0.8733853 0.2296582 0.7703418  0.072416079
## 80    0.005     0.3539287     0.2364617 0.5584370 0.4415630 -0.204508321
## 81    0.005     0.3043191     0.3345255 0.4851394 0.5148606 -0.180820269
## 82    0.005     0.3466176     0.9922840 0.1850131 0.8149869  0.161604524
## 83    0.005     0.3466322     1.0461483 0.1540090 0.8459910  0.192623241
## 84    0.005     0.3861873     0.9340806 0.2335734 0.7664266  0.152613813
## 85    0.005     0.2510176     0.3712978 0.4406049 0.5593951 -0.189587338
## 86    0.005     0.3038270     0.4939395 0.4092666 0.5907334 -0.105439573
## 87    0.005     0.3348725     0.7824376 0.2890425 0.7109575  0.045829997
## 88    0.005     0.3883738     0.9529528 0.2241331 0.7758669  0.164240731
## 89    0.005     0.3298538     1.0880884 0.1224074 0.8775926  0.207446412
## 90    0.005     0.2977581     0.4592909 0.4224089 0.5775911 -0.124650805
## 91    0.005     0.3404793     1.0664561 0.1395526 0.8604474  0.200926688
## 92    0.005     0.3292815     1.0890609 0.1215966 0.8784034  0.207684868
## 93    0.005     0.3483122     1.0036812 0.1792489 0.8207511  0.169063265
## 94    0.005     0.2338848     0.4401550 0.3993364 0.6006636 -0.165451633
## 95    0.005     0.2471309     0.5248099 0.3670294 0.6329706 -0.119898479
## 96    0.005     0.3446630     1.0541123 0.1485324 0.8514676  0.196130551
## 97    0.005     0.3873290     0.9421680 0.2296337 0.7703663  0.157695285
## 98    0.005     0.3876022     0.9855795 0.2055485 0.7944515  0.182053727
## 99    0.005     0.2392063     0.4083108 0.4169811 0.5830189 -0.177774822
## 100   0.005     0.3118593     0.7269708 0.3051180 0.6948820  0.006741281
## 101   0.005     0.2449229     0.5109697 0.3722502 0.6277498 -0.127327346
## 102   0.005     0.3030336     0.4900780 0.4106699 0.5893301 -0.107636349
## 103   0.005     0.3740378     0.2094174 0.5820236 0.4179764 -0.207985832
## 104   0.005     0.3451236     1.0524398 0.1497031 0.8502969  0.195420485
## 105   0.005     0.2325244     0.4730284 0.3834130 0.6165870 -0.150888562
## 106   0.005     0.2435624     0.5005762 0.3763276 0.6236724 -0.132765138
## 107   0.005     0.2940830     0.4109778 0.4433052 0.5566948 -0.149222156
## 108   0.005     0.3325342     0.2698168 0.5311149 0.4688851 -0.198580733
## 109   0.005     0.3163174     0.7378315 0.3020291 0.6979709  0.014288255
## 110   0.005     0.3147957     0.5387115 0.3938855 0.6061145 -0.079089778
## 111   0.005     0.2366407     0.4206360 0.4098710 0.5901290 -0.173230292
## 112   0.005     0.3174888     0.9070321 0.2190405 0.7809595  0.098448324
## 113   0.005     0.2401767     0.4043328 0.4193549 0.5806451 -0.179178187
## 114   0.005     0.3274590     0.9305238 0.2108993 0.7891007  0.116559727
## 115   0.005     0.3833648     0.9191372 0.2404723 0.7595277  0.142892523
## 116   0.005     0.3872901     0.9879192 0.2040929 0.7959071  0.183197220
## 117   0.005     0.3847275     0.9257855 0.2374613 0.7625387  0.147266291
## 118   0.005     0.2782858     0.6701394 0.3158905 0.6841095 -0.037604706
## 119   0.005     0.2327265     0.4564764 0.3911636 0.6088364 -0.158437064
## 120   0.005     0.3877921     0.9462566 0.2275826 0.7724174  0.160209500
## 121   0.005     0.3492835     1.0217386 0.1692910 0.8307090  0.179992489
## 122   0.005     0.2941488     0.4156790 0.4411129 0.5588871 -0.146964072
## 123   0.005     0.3360499     0.7852657 0.2881953 0.7118047  0.047854585
## 124   0.005     0.3365538     1.0754676 0.1326229 0.8673771  0.203930859
## 125   0.005     0.3662287     0.8615194 0.2634130 0.7365870  0.102815741
## 126   0.005     0.3461295     0.9896934 0.1862720 0.8137280  0.159857556
## 127   0.005     0.2464857     0.5698953 0.3461941 0.6538059 -0.099708459
## 128   0.005     0.2673401     0.6060150 0.3399832 0.6600168 -0.072643021
## 129   0.005     0.3289472     0.9342463 0.2095405 0.7904595  0.119406651
## 130   0.005     0.3402372     0.7953544 0.2851423 0.7148577  0.055094914
## 131   0.005     0.3632970     0.8534693 0.2662616 0.7337384  0.097035395
## 132   0.005     0.3682961     0.8673644 0.2612991 0.7387009  0.106996910
## 133   0.005     0.3489140     1.0101026 0.1758292 0.8241708  0.173084854
## 134   0.005     0.2601434     0.5815560 0.3476150 0.6523850 -0.087471621
## 135   0.005     0.2869098     0.6927203 0.3095323 0.6904677 -0.022622531
## 136   0.005     0.3212660     0.9156908 0.2161210 0.7838790  0.105144978
## 137   0.005     0.3484312     0.8153404 0.2789286 0.7210714  0.069502672
## 138   0.005     0.2353500     0.5116058 0.3671510 0.6328490 -0.131800981
## 139   0.005     0.3136054     0.7580132 0.2911898 0.7088102  0.022415643
## 140   0.005     0.3778290     0.8972226 0.2498051 0.7501949  0.128023938
## 141   0.005     0.3309612     0.5914027 0.3772897 0.6227103 -0.046328479
## 142   0.005     0.2714304     0.6513172 0.3212645 0.6787355 -0.049834102
## 143   0.005     0.2352319     0.4292538 0.4051133 0.5948867 -0.169881393
## 144   0.005     0.3301437     1.0875908 0.1228210 0.8771790  0.207322658
## 145   0.005     0.3355528     0.8098456 0.2759359 0.7240641  0.059616877
## 146   0.005     0.2515585     0.3699700 0.4415170 0.5584830 -0.189958433
## 147   0.005     0.2471471     0.5249026 0.3669952 0.6330048 -0.119848015
## 148   0.005     0.2662599     0.6025104 0.3410562 0.6589438 -0.074796253
## 149   0.005     0.3464688     1.0469075 0.1534976 0.8465024  0.192971253
## 150   0.005     0.3042273     0.4958397 0.4085811 0.5914189 -0.104353747
## 151   0.005     0.2467337     0.5225075 0.3678824 0.6321176 -0.121148624
## 152   0.005     0.2826016     0.6815713 0.3126626 0.6873374 -0.030060995
## 153   0.005     0.2542265     0.3636917 0.4458899 0.5541101 -0.191663337
## 154   0.005     0.2520426     0.3687985 0.4423254 0.5576746 -0.190282776
## 155   0.005     0.3328317     0.8033819 0.2779033 0.7220967  0.054928448
## 156   0.005     0.2366280     0.4207062 0.4098316 0.5901684 -0.173203585
## 157   0.005     0.2985591     0.8659268 0.2318832 0.7681168  0.066675935
## 158   0.005     0.3544725     0.2356880 0.5590955 0.4409045 -0.204623045
## 159   0.005     0.2493061     0.5364251 0.3628144 0.6371856 -0.113508312
## 160   0.005     0.3357385     0.2643684 0.5354260 0.4645740 -0.199687453
## 161   0.005     0.2374831     0.4162258 0.4123732 0.5876268 -0.174890109
## 162   0.005     0.3212602     0.9156775 0.2161255 0.7838745  0.105134698
## 163   0.005     0.3534223     0.8537305 0.2618294 0.7381706  0.091592963
## 164   0.005     0.3669394     0.2186373 0.5738575 0.4261425 -0.206918064
## 165   0.005     0.2956313     0.6862685 0.3166159 0.6833841 -0.020984576
## 166   0.005     0.3470663     0.9948582 0.1837439 0.8162561  0.163322454
## 167   0.005     0.2927788     0.6788466 0.3187165 0.6812835 -0.025937772
## 168   0.005     0.3217317     0.2900237 0.5156967 0.4843033 -0.193964951
## 169   0.005     0.3465333     0.8363982 0.2675782 0.7324218  0.078955078
## 170   0.005     0.2579885     0.5736082 0.3501751 0.6498249 -0.092186607
## 171   0.005     0.3419176     0.6232169 0.3677180 0.6322820 -0.025800352
## 172   0.005     0.2944453     0.4242934 0.4371919 0.5628081 -0.142746596
## 173   0.005     0.2617335     0.6228116 0.3296129 0.6703871 -0.067879394
## 174   0.005     0.2497191     0.5384769 0.3620845 0.6379155 -0.112365385
## 175   0.005     0.2384175     0.5319450 0.3594211 0.6405789 -0.121003607
## 176   0.005     0.2329676     0.4855680 0.3778690 0.6221310 -0.144901352
## 177   0.005     0.3476312     0.8391148 0.2666942 0.7333058  0.080936962
## 178   0.005     0.2497154     0.3745796 0.4383698 0.5616302 -0.188654353
## 179   0.005     0.2763432     0.6648955 0.3173793 0.6826207 -0.041036085
## 180   0.005     0.3345436     0.6020426 0.3740635 0.6259365 -0.039519804
## 181   0.005     0.3056397     0.3301358 0.4879403 0.5120597 -0.182300562
## 182   0.005     0.2354031     0.5120242 0.3669867 0.6330133 -0.131583598
## 183   0.005     0.3031054     0.3388380 0.4824330 0.5175670 -0.179327672
## 184   0.005     0.3150384     0.3046157 0.5051503 0.4948497 -0.190111835
## 185   0.005     0.2324881     0.4688537 0.3853197 0.6146803 -0.152831654
## 186   0.005     0.3222767     0.5642795 0.3856797 0.6143203 -0.063403007
## 187   0.005     0.3137333     0.7583162 0.2911034 0.7088966  0.022629957
## 188   0.005     0.3859075     0.9960768 0.1988803 0.8011197  0.187027180
## 189   0.005     0.3405472     0.6193450 0.3688724 0.6311276 -0.028325182
## 190   0.005     0.2440433     0.3906991 0.4277897 0.5722103 -0.183746333
## 191   0.005     0.2726457     0.6225710 0.3349933 0.6650067 -0.062347598
## 192   0.005     0.2550663     0.5621894 0.3539333 0.6460667 -0.098867072
## 193   0.005     0.2589487     0.6140732 0.3322394 0.6677606 -0.073290641
## 194   0.005     0.3492765     1.0183475 0.1712435 0.8287565  0.178033032
## 195   0.005     0.2380856     0.5300163 0.3601327 0.6398673 -0.122047108
## 196   0.005     0.2886570     0.6679318 0.3218177 0.6781823 -0.033160728
## 197   0.005     0.3213493     0.7499900 0.2985455 0.7014545  0.022803795
## 198   0.005     0.3300681     0.7709169 0.2924586 0.7075414  0.037609436
## 199   0.005     0.3674305     0.2179893 0.5744274 0.4255726 -0.206996888
## 200   0.005     0.3857984     0.9966228 0.1985237 0.8014763  0.187274614
##         e_dual_0
## 1    0.196816664
## 2    0.195213887
## 3   -0.191748265
## 4    0.123649764
## 5   -0.154763080
## 6   -0.080691992
## 7   -0.073212639
## 8    0.200805121
## 9    0.133987104
## 10  -0.034786918
## 11   0.194353009
## 12  -0.174345776
## 13   0.129565362
## 14   0.144629881
## 15  -0.165065824
## 16   0.042036952
## 17   0.120126287
## 18  -0.151913091
## 19   0.201671483
## 20  -0.108044714
## 21  -0.202081443
## 22   0.197380539
## 23   0.159371796
## 24   0.167486415
## 25  -0.172100408
## 26  -0.183454370
## 27   0.154131632
## 28  -0.097708197
## 29   0.196740274
## 30   0.112833510
## 31   0.190924511
## 32  -0.203513696
## 33  -0.120295285
## 34  -0.076567094
## 35   0.196882469
## 36  -0.046483761
## 37  -0.183042405
## 38   0.108598206
## 39   0.081468618
## 40   0.198730450
## 41  -0.136679421
## 42   0.095921278
## 43  -0.182049288
## 44  -0.193805163
## 45   0.107996042
## 46   0.198095729
## 47   0.033563670
## 48  -0.204506037
## 49  -0.050518416
## 50   0.054785371
## 51  -0.201856236
## 52   0.101673837
## 53   0.191370771
## 54  -0.125933112
## 55   0.155689232
## 56   0.189412011
## 57   0.142293278
## 58  -0.086322050
## 59  -0.095287583
## 60   0.201169927
## 61   0.064938222
## 62   0.181863142
## 63   0.210076727
## 64  -0.138737577
## 65   0.197008490
## 66  -0.140667450
## 67  -0.095833812
## 68   0.060626062
## 69   0.075688980
## 70   0.051946268
## 71   0.032717364
## 72   0.195282063
## 73  -0.198458443
## 74  -0.165868084
## 75   0.119957811
## 76   0.188389397
## 77   0.175942816
## 78   0.038961025
## 79   0.103043498
## 80  -0.205101274
## 81  -0.180335052
## 82   0.177297082
## 83   0.200157289
## 84   0.167654089
## 85  -0.188097319
## 86  -0.096793941
## 87   0.071480023
## 88   0.177085896
## 89   0.210495822
## 90  -0.118300236
## 91   0.206008747
## 92   0.210657515
## 93   0.182930089
## 94  -0.160508544
## 95  -0.108160724
## 96   0.202644753
## 97   0.171801718
## 98   0.191127992
## 99  -0.174708058
## 100  0.032088832
## 101 -0.116780090
## 102 -0.099252109
## 103 -0.208558971
## 104  0.202142937
## 105 -0.143558587
## 106 -0.123096208
## 107 -0.145717012
## 108 -0.199068277
## 109  0.039860676
## 110 -0.067403036
## 111 -0.169493007
## 112  0.126072524
## 113 -0.176312333
## 114  0.141423095
## 115  0.159609523
## 116  0.192012051
## 117  0.163246716
## 118 -0.013970113
## 119 -0.152360033
## 120  0.173839255
## 121  0.191029641
## 122 -0.143208137
## 123  0.073461037
## 124  0.208090541
## 125  0.124932387
## 126  0.175965345
## 127 -0.083910528
## 128 -0.054001831
## 129  0.143786805
## 130  0.080496779
## 131  0.119730871
## 132  0.128663544
## 133  0.185931814
## 134 -0.070828995
## 135  0.002252669
## 136  0.131811811
## 137  0.094268979
## 138 -0.121243189
## 139  0.049203005
## 140  0.147027708
## 141 -0.031307660
## 142 -0.027418264
## 143 -0.165632941
## 144  0.210411825
## 145  0.085781540
## 146 -0.188513055
## 147 -0.108102226
## 148 -0.056433456
## 149  0.200405075
## 150 -0.095579180
## 151 -0.109610161
## 152 -0.005766018
## 153 -0.190418411
## 154 -0.188876125
## 155  0.081285169
## 156 -0.169462276
## 157  0.097809973
## 158 -0.205216455
## 159 -0.100760466
## 160 -0.200205658
## 161 -0.171401037
## 162  0.131803057
## 163  0.115559883
## 164 -0.207505211
## 165  0.002884399
## 166  0.178602090
## 167 -0.002436898
## 168 -0.194279638
## 169  0.103976385
## 170 -0.076216642
## 171 -0.009065162
## 172 -0.138514685
## 173 -0.047575443
## 174 -0.099438602
## 175 -0.108633900
## 176 -0.136563012
## 177  0.105809014
## 178 -0.187050576
## 179 -0.017725265
## 180 -0.023893964
## 181 -0.181923913
## 182 -0.120989060
## 183 -0.178728985
## 184 -0.190234069
## 185 -0.145826622
## 186 -0.050040775
## 187  0.049419573
## 188  0.194957145
## 189 -0.011782594
## 190 -0.181511188
## 191 -0.042435679
## 192 -0.083877242
## 193 -0.053687413
## 194  0.189591043
## 195 -0.109851009
## 196 -0.010250564
## 197  0.048535570
## 198  0.063375576
## 199 -0.207583261
## 200  0.195146555
## 
## $values
## $values$divergencekl
## [1] -177.9593
## 
## $values$iterations
## [1] 94
## 
## $values$message
## [1] &quot;relative convergence (4)&quot;
## 
## 
## $tol
## [1] 1e-10
## 
## $v
##           [,1] [,2]       [,3]
## [1,] 0.2160606    0 -0.2160606
## 
## $lambda
##   (Intercept) Dcollege  Totalincome    Dunemp
## 1    962.9111 2136.314 -0.005113246 -1048.040
## 0   1189.9970 2308.253 -0.005420460 -1161.821
## 
## $checkrestrictions
## $checkrestrictions$g1
##        [,1]
##   [1,]    0
##   [2,]    0
##   [3,]    0
##   [4,]    0
##   [5,]    0
##   [6,]    0
##   [7,]    0
##   [8,]    0
##   [9,]    0
##  [10,]    0
##  [11,]    0
##  [12,]    0
##  [13,]    0
##  [14,]    0
##  [15,]    0
##  [16,]    0
##  [17,]    0
##  [18,]    0
##  [19,]    0
##  [20,]    0
##  [21,]    0
##  [22,]    0
##  [23,]    0
##  [24,]    0
##  [25,]    0
##  [26,]    0
##  [27,]    0
##  [28,]    0
##  [29,]    0
##  [30,]    0
##  [31,]    0
##  [32,]    0
##  [33,]    0
##  [34,]    0
##  [35,]    0
##  [36,]    0
##  [37,]    0
##  [38,]    0
##  [39,]    0
##  [40,]    0
##  [41,]    0
##  [42,]    0
##  [43,]    0
##  [44,]    0
##  [45,]    0
##  [46,]    0
##  [47,]    0
##  [48,]    0
##  [49,]    0
##  [50,]    0
##  [51,]    0
##  [52,]    0
##  [53,]    0
##  [54,]    0
##  [55,]    0
##  [56,]    0
##  [57,]    0
##  [58,]    0
##  [59,]    0
##  [60,]    0
##  [61,]    0
##  [62,]    0
##  [63,]    0
##  [64,]    0
##  [65,]    0
##  [66,]    0
##  [67,]    0
##  [68,]    0
##  [69,]    0
##  [70,]    0
##  [71,]    0
##  [72,]    0
##  [73,]    0
##  [74,]    0
##  [75,]    0
##  [76,]    0
##  [77,]    0
##  [78,]    0
##  [79,]    0
##  [80,]    0
##  [81,]    0
##  [82,]    0
##  [83,]    0
##  [84,]    0
##  [85,]    0
##  [86,]    0
##  [87,]    0
##  [88,]    0
##  [89,]    0
##  [90,]    0
##  [91,]    0
##  [92,]    0
##  [93,]    0
##  [94,]    0
##  [95,]    0
##  [96,]    0
##  [97,]    0
##  [98,]    0
##  [99,]    0
## [100,]    0
## [101,]    0
## [102,]    0
## [103,]    0
## [104,]    0
## [105,]    0
## [106,]    0
## [107,]    0
## [108,]    0
## [109,]    0
## [110,]    0
## [111,]    0
## [112,]    0
## [113,]    0
## [114,]    0
## [115,]    0
## [116,]    0
## [117,]    0
## [118,]    0
## [119,]    0
## [120,]    0
## [121,]    0
## [122,]    0
## [123,]    0
## [124,]    0
## [125,]    0
## [126,]    0
## [127,]    0
## [128,]    0
## [129,]    0
## [130,]    0
## [131,]    0
## [132,]    0
## [133,]    0
## [134,]    0
## [135,]    0
## [136,]    0
## [137,]    0
## [138,]    0
## [139,]    0
## [140,]    0
## [141,]    0
## [142,]    0
## [143,]    0
## [144,]    0
## [145,]    0
## [146,]    0
## [147,]    0
## [148,]    0
## [149,]    0
## [150,]    0
## [151,]    0
## [152,]    0
## [153,]    0
## [154,]    0
## [155,]    0
## [156,]    0
## [157,]    0
## [158,]    0
## [159,]    0
## [160,]    0
## [161,]    0
## [162,]    0
## [163,]    0
## [164,]    0
## [165,]    0
## [166,]    0
## [167,]    0
## [168,]    0
## [169,]    0
## [170,]    0
## [171,]    0
## [172,]    0
## [173,]    0
## [174,]    0
## [175,]    0
## [176,]    0
## [177,]    0
## [178,]    0
## [179,]    0
## [180,]    0
## [181,]    0
## [182,]    0
## [183,]    0
## [184,]    0
## [185,]    0
## [186,]    0
## [187,]    0
## [188,]    0
## [189,]    0
## [190,]    0
## [191,]    0
## [192,]    0
## [193,]    0
## [194,]    0
## [195,]    0
## [196,]    0
## [197,]    0
## [198,]    0
## [199,]    0
## [200,]    0
## 
## $checkrestrictions$g2
##        [,1] [,2]
##   [1,]    0    0
##   [2,]    0    0
##   [3,]    0    0
##   [4,]    0    0
##   [5,]    0    0
##   [6,]    0    0
##   [7,]    0    0
##   [8,]    0    0
##   [9,]    0    0
##  [10,]    0    0
##  [11,]    0    0
##  [12,]    0    0
##  [13,]    0    0
##  [14,]    0    0
##  [15,]    0    0
##  [16,]    0    0
##  [17,]    0    0
##  [18,]    0    0
##  [19,]    0    0
##  [20,]    0    0
##  [21,]    0    0
##  [22,]    0    0
##  [23,]    0    0
##  [24,]    0    0
##  [25,]    0    0
##  [26,]    0    0
##  [27,]    0    0
##  [28,]    0    0
##  [29,]    0    0
##  [30,]    0    0
##  [31,]    0    0
##  [32,]    0    0
##  [33,]    0    0
##  [34,]    0    0
##  [35,]    0    0
##  [36,]    0    0
##  [37,]    0    0
##  [38,]    0    0
##  [39,]    0    0
##  [40,]    0    0
##  [41,]    0    0
##  [42,]    0    0
##  [43,]    0    0
##  [44,]    0    0
##  [45,]    0    0
##  [46,]    0    0
##  [47,]    0    0
##  [48,]    0    0
##  [49,]    0    0
##  [50,]    0    0
##  [51,]    0    0
##  [52,]    0    0
##  [53,]    0    0
##  [54,]    0    0
##  [55,]    0    0
##  [56,]    0    0
##  [57,]    0    0
##  [58,]    0    0
##  [59,]    0    0
##  [60,]    0    0
##  [61,]    0    0
##  [62,]    0    0
##  [63,]    0    0
##  [64,]    0    0
##  [65,]    0    0
##  [66,]    0    0
##  [67,]    0    0
##  [68,]    0    0
##  [69,]    0    0
##  [70,]    0    0
##  [71,]    0    0
##  [72,]    0    0
##  [73,]    0    0
##  [74,]    0    0
##  [75,]    0    0
##  [76,]    0    0
##  [77,]    0    0
##  [78,]    0    0
##  [79,]    0    0
##  [80,]    0    0
##  [81,]    0    0
##  [82,]    0    0
##  [83,]    0    0
##  [84,]    0    0
##  [85,]    0    0
##  [86,]    0    0
##  [87,]    0    0
##  [88,]    0    0
##  [89,]    0    0
##  [90,]    0    0
##  [91,]    0    0
##  [92,]    0    0
##  [93,]    0    0
##  [94,]    0    0
##  [95,]    0    0
##  [96,]    0    0
##  [97,]    0    0
##  [98,]    0    0
##  [99,]    0    0
## [100,]    0    0
## [101,]    0    0
## [102,]    0    0
## [103,]    0    0
## [104,]    0    0
## [105,]    0    0
## [106,]    0    0
## [107,]    0    0
## [108,]    0    0
## [109,]    0    0
## [110,]    0    0
## [111,]    0    0
## [112,]    0    0
## [113,]    0    0
## [114,]    0    0
## [115,]    0    0
## [116,]    0    0
## [117,]    0    0
## [118,]    0    0
## [119,]    0    0
## [120,]    0    0
## [121,]    0    0
## [122,]    0    0
## [123,]    0    0
## [124,]    0    0
## [125,]    0    0
## [126,]    0    0
## [127,]    0    0
## [128,]    0    0
## [129,]    0    0
## [130,]    0    0
## [131,]    0    0
## [132,]    0    0
## [133,]    0    0
## [134,]    0    0
## [135,]    0    0
## [136,]    0    0
## [137,]    0    0
## [138,]    0    0
## [139,]    0    0
## [140,]    0    0
## [141,]    0    0
## [142,]    0    0
## [143,]    0    0
## [144,]    0    0
## [145,]    0    0
## [146,]    0    0
## [147,]    0    0
## [148,]    0    0
## [149,]    0    0
## [150,]    0    0
## [151,]    0    0
## [152,]    0    0
## [153,]    0    0
## [154,]    0    0
## [155,]    0    0
## [156,]    0    0
## [157,]    0    0
## [158,]    0    0
## [159,]    0    0
## [160,]    0    0
## [161,]    0    0
## [162,]    0    0
## [163,]    0    0
## [164,]    0    0
## [165,]    0    0
## [166,]    0    0
## [167,]    0    0
## [168,]    0    0
## [169,]    0    0
## [170,]    0    0
## [171,]    0    0
## [172,]    0    0
## [173,]    0    0
## [174,]    0    0
## [175,]    0    0
## [176,]    0    0
## [177,]    0    0
## [178,]    0    0
## [179,]    0    0
## [180,]    0    0
## [181,]    0    0
## [182,]    0    0
## [183,]    0    0
## [184,]    0    0
## [185,]    0    0
## [186,]    0    0
## [187,]    0    0
## [188,]    0    0
## [189,]    0    0
## [190,]    0    0
## [191,]    0    0
## [192,]    0    0
## [193,]    0    0
## [194,]    0    0
## [195,]    0    0
## [196,]    0    0
## [197,]    0    0
## [198,]    0    0
## [199,]    0    0
## [200,]    0    0
## 
## $checkrestrictions$g3
##             y_factor1 y_factor0
## (Intercept)     0.000     0.000
## Dcollege        0.000     0.000
## Totalincome     0.005     0.034
## Dunemp          0.000     0.000
## 
## 
## $cross_moments_hp
##                    1         0
## (Intercept)     0.31      0.69
## Dcollege        0.17      0.45
## Totalincome 94640.65 197483.48
## Dunemp          0.15      0.28
## 
## $cross_moments_hs
##             1          0         
## (Intercept)       0.31       0.69
## Dcollege          0.17       0.45
## Totalincome   94640.65  197483.44
## Dunemp            0.15       0.28
## 
## $J
## [1] 2
## 
## $fn
## datahp$poor_liq ~ Dcollege + Totalincome + Dunemp
## 
## attr(,&quot;class&quot;)
## [1] &quot;kl&quot;</code></pre>
<p>The summary and the plot options are the same as in the previous
case.</p>
</div>
<div id="another-option-when-we-do-not-have-prior-information-ei_gme" class="section level1">
<h1>3.2.1 Another option when we do not have prior information:
ei_gme()</h1>
<p>In the case the user does not have prior information the function
ei_gme() is also available. This function is provided for cases where
the user may prefer using GME (Generalized maximum entropy) if the user
has a big volume of data, because it is a more efficient function in
computational terms than ei_gce() with the uniform distribution as
prior. Both functions should provide the same result.</p>
<p>ei_gme() applies the Shannon entropy function to the optimization. It
is adequate when you cannot assume anything about the distribution of
<span class="math inline">\(Y\)</span>. So the starting point is the
uniform distribution.</p>
<p>Using the same example, our variable of interest is the poverty rate
in terms of wealth and our independent variables are the income and
dummies for unemployment and college studies. In this case, we need to
define the function</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>result3 <span class="ot">&lt;-</span> <span class="fu">ei_gme</span> (fn,datahp,datahs,<span class="at">weights=</span>w)</span></code></pre></div>
<p>The function will produce the same output as ei_gce(), see the
previous section for further details.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>result3</span></code></pre></div>
<pre><code>## $estimations
##     weights predictions_1 predictions_0  p_dual_1  p_dual_0     e_dual_1
## 1     0.005     0.3482708     1.0365048 0.1603118 0.8396882  0.187958981
## 2     0.005     0.3857587     0.9968178 0.1983961 0.8016039  0.187362592
## 3     0.005     0.3173373     0.2993648 0.5088869 0.4911131 -0.191549608
## 4     0.005     0.3158818     0.9034147 0.2202347 0.7797653  0.095647047
## 5     0.005     0.2329584     0.4517757 0.3934611 0.6065389 -0.160502696
## 6     0.005     0.2476943     0.5746947 0.3446131 0.6553869 -0.096918783
## 7     0.005     0.2506218     0.5857494 0.3410377 0.6589623 -0.090415952
## 8     0.005     0.3461908     1.0481479 0.1526572 0.8473428  0.193533682
## 9     0.005     0.3226841     0.9190086 0.2149783 0.7850217  0.107705859
## 10    0.005     0.2762948     0.6334307 0.3317821 0.6682179 -0.055487296
## 11    0.005     0.3489346     1.0299689 0.1643841 0.8356159  0.184550543
## 12    0.005     0.2389998     0.4091959 0.4164582 0.5835418 -0.177458386
## 13    0.005     0.3615026     0.8753047 0.2542606 0.7457394  0.107242006
## 14    0.005     0.3294736     0.9355817 0.2090479 0.7909521  0.120425680
## 15    0.005     0.2964826     0.3716957 0.4632385 0.5367615 -0.166755935
## 16    0.005     0.3175743     0.7408775 0.3011592 0.6988408  0.016415067
## 17    0.005     0.3560947     0.8606809 0.2594453 0.7405547  0.096649400
## 18    0.005     0.2942369     0.3990382 0.4490487 0.5509513 -0.154811769
## 19    0.005     0.3455252     1.0509007 0.1507707 0.8492293  0.194754511
## 20    0.005     0.2471630     0.5249934 0.3669617 0.6330383 -0.119798609
## 21    0.005     0.3417449     0.2546508 0.5432678 0.4567322 -0.201522857
## 22    0.005     0.3842932     1.0032909 0.1940896 0.8059104  0.190203544
## 23    0.005     0.3382041     0.9597578 0.1996138 0.8003862  0.138590326
## 24    0.005     0.3861359     0.9337598 0.2337265 0.7662735  0.152409340
## 25    0.005     0.2378184     0.4145816 0.4133178 0.5866822 -0.175499413
## 26    0.005     0.2458231     0.3852596 0.4312859 0.5687141 -0.185462864
## 27    0.005     0.3352229     0.9509667 0.2031647 0.7968353  0.132058152
## 28    0.005     0.2416923     0.5489803 0.3533113 0.6466887 -0.111619006
## 29    0.005     0.3482976     1.0362956 0.1604447 0.8395553  0.187852907
## 30    0.005     0.3518128     0.8496149 0.2632186 0.7367814  0.088594237
## 31    0.005     0.3492858     1.0214876 0.1694369 0.8305631  0.179848928
## 32    0.005     0.3470707     0.2464819 0.5500044 0.4499956 -0.202933695
## 33    0.005     0.2441365     0.5052137 0.3744908 0.6255092 -0.130354312
## 34    0.005     0.2578510     0.5730886 0.3503440 0.6496560 -0.092493042
## 35    0.005     0.3482475     1.0366853 0.1601971 0.8398029  0.188050377
## 36    0.005     0.2622387     0.6243671 0.3291490 0.6708510 -0.066910314
## 37    0.005     0.3066469     0.3269657 0.4899919 0.5100081 -0.183344969
## 38    0.005     0.3568756     0.8365688 0.2720293 0.7279707  0.084846263
## 39    0.005     0.3408158     0.7967537 0.2847148 0.7152852  0.056101053
## 40    0.005     0.3474525     1.0419027 0.1568277 0.8431723  0.190624826
## 41    0.005     0.2329563     0.4853630 0.3779574 0.6220426 -0.145001088
## 42    0.005     0.2972919     0.8632481 0.2326728 0.7673272  0.064619130
## 43    0.005     0.3057491     0.3297840 0.4881667 0.5118333 -0.182417560
## 44    0.005     0.3208498     0.2918340 0.5143609 0.4856391 -0.193511056
## 45    0.005     0.3054031     0.8804983 0.2274974 0.7725026  0.077905670
## 46    0.005     0.3477571     1.0400775 0.1580182 0.8419818  0.189738871
## 47    0.005     0.3127011     0.7290297 0.3045337 0.6954663  0.008167423
## 48    0.005     0.3512238     0.2403536 0.5551403 0.4448597 -0.203916529
## 49    0.005     0.2689094     0.6110199 0.3384614 0.6615386 -0.069552049
## 50    0.005     0.2702645     0.8061787 0.2486062 0.7513938  0.021658292
## 51    0.005     0.3409703     0.2558717 0.5422720 0.4577280 -0.201301790
## 52    0.005     0.3451504     0.8329973 0.2686764 0.7313236  0.076473968
## 53    0.005     0.3875206     0.9862176 0.2051532 0.7948468  0.182367453
## 54    0.005     0.2344432     0.5038115 0.3702552 0.6297448 -0.135812006
## 55    0.005     0.3817620     0.9121441 0.2435451 0.7564549  0.138216914
## 56    0.005     0.3492698     1.0179318 0.1714801 0.8285199  0.177789645
## 57    0.005     0.3684233     0.8957111 0.2465821 0.7534179  0.121841165
## 58    0.005     0.3074432     0.5101624 0.4035156 0.5964844 -0.096072326
## 59    0.005     0.2510604     0.5448787 0.3598335 0.6401665 -0.108773109
## 60    0.005     0.3459215     1.0492956 0.1518742 0.8481258  0.194047295
## 61    0.005     0.2768012     0.8200921 0.2448457 0.7551543  0.031955555
## 62    0.005     0.3480428     1.0014619 0.1804012 0.8195988  0.167641638
## 63    0.005     0.3312516     1.0856548 0.1244219 0.8755781  0.206829728
## 64    0.005     0.2944261     0.4238896 0.4373728 0.5626272 -0.142946728
## 65    0.005     0.3845735     1.0021493 0.1948592 0.8051408  0.189714375
## 66    0.005     0.2326497     0.4782693 0.3810631 0.6189369 -0.148413370
## 67    0.005     0.2423033     0.5518574 0.3523086 0.6476914 -0.110005275
## 68    0.005     0.3203926     0.7740449 0.2865810 0.7134190  0.033811649
## 69    0.005     0.3294483     0.7953811 0.2803077 0.7196923  0.049140530
## 70    0.005     0.3152272     0.7618539 0.2900922 0.7099078  0.025135065
## 71    0.005     0.3039818     0.7350242 0.2976930 0.7023070  0.006288895
## 72    0.005     0.3857184     0.9970153 0.1982667 0.8017333  0.187451705
## 73    0.005     0.3309328     0.2726196 0.5289220 0.4710780 -0.197989134
## 74    0.005     0.2353072     0.4287394 0.4053924 0.5946076 -0.170085197
## 75    0.005     0.3134228     0.8979426 0.2220149 0.7779851  0.091407810
## 76    0.005     0.3492084     1.0155833 0.1728060 0.8271940  0.176402375
## 77    0.005     0.3461209     0.9896497 0.1862930 0.8137070  0.159827899
## 78    0.005     0.2603058     0.7846239 0.2543366 0.7456634  0.005969233
## 79    0.005     0.3020740     0.8733848 0.2296583 0.7703417  0.072415678
## 80    0.005     0.3539286     0.2364618 0.5584369 0.4415631 -0.204508337
## 81    0.005     0.3043189     0.3345257 0.4851392 0.5148608 -0.180820297
## 82    0.005     0.3466175     0.9922838 0.1850132 0.8149868  0.161604373
## 83    0.005     0.3466321     1.0461483 0.1540089 0.8459911  0.192623187
## 84    0.005     0.3861871     0.9340807 0.2335733 0.7664267  0.152613754
## 85    0.005     0.2510175     0.3712976 0.4406050 0.5593950 -0.189587415
## 86    0.005     0.3038267     0.4939398 0.4092662 0.5907338 -0.105439587
## 87    0.005     0.3348722     0.7824374 0.2890424 0.7109576  0.045829797
## 88    0.005     0.3883736     0.9529529 0.2241329 0.7758671  0.164240688
## 89    0.005     0.3298537     1.0880885 0.1224074 0.8775926  0.207446397
## 90    0.005     0.2977577     0.4592912 0.4224086 0.5775914 -0.124650827
## 91    0.005     0.3404792     1.0664561 0.1395526 0.8604474  0.200926657
## 92    0.005     0.3292814     1.0890610 0.1215965 0.8784035  0.207684854
## 93    0.005     0.3483120     1.0036811 0.1792489 0.8207511  0.169063138
## 94    0.005     0.2338846     0.4401549 0.3993364 0.6006636 -0.165451761
## 95    0.005     0.2471308     0.5248095 0.3670295 0.6329705 -0.119898686
## 96    0.005     0.3446629     1.0541124 0.1485324 0.8514676  0.196130507
## 97    0.005     0.3873288     0.9421681 0.2296336 0.7703664  0.157695234
## 98    0.005     0.3876021     0.9855796 0.2055484 0.7944516  0.182053707
## 99    0.005     0.2392062     0.4083107 0.4169811 0.5830189 -0.177774926
## 100   0.005     0.3118591     0.7269706 0.3051180 0.6948820  0.006741048
## 101   0.005     0.2449228     0.5109693 0.3722504 0.6277496 -0.127327544
## 102   0.005     0.3030332     0.4900783 0.4106696 0.5893304 -0.107636364
## 103   0.005     0.3740377     0.2094175 0.5820236 0.4179764 -0.207985845
## 104   0.005     0.3451235     1.0524398 0.1497031 0.8502969  0.195420439
## 105   0.005     0.2325242     0.4730283 0.3834129 0.6165871 -0.150888715
## 106   0.005     0.2435624     0.5005759 0.3763277 0.6236723 -0.132765329
## 107   0.005     0.2940827     0.4109781 0.4433049 0.5566951 -0.149222185
## 108   0.005     0.3325340     0.2698169 0.5311148 0.4688852 -0.198580754
## 109   0.005     0.3163172     0.7378313 0.3020291 0.6979709  0.014288028
## 110   0.005     0.3147953     0.5387119 0.3938851 0.6061149 -0.079089779
## 111   0.005     0.2366406     0.4206359 0.4098710 0.5901290 -0.173230405
## 112   0.005     0.3174885     0.9070317 0.2190406 0.7809594  0.098447985
## 113   0.005     0.2401766     0.4043327 0.4193549 0.5806451 -0.179178287
## 114   0.005     0.3274588     0.9305235 0.2108994 0.7891006  0.116559438
## 115   0.005     0.3833646     0.9191373 0.2404722 0.7595278  0.142892450
## 116   0.005     0.3872899     0.9879193 0.2040927 0.7959073  0.183197201
## 117   0.005     0.3847274     0.9257855 0.2374611 0.7625389  0.147266225
## 118   0.005     0.2782854     0.6701394 0.3158903 0.6841097 -0.037604936
## 119   0.005     0.2327264     0.4564763 0.3911636 0.6088364 -0.158437205
## 120   0.005     0.3877920     0.9462567 0.2275825 0.7724175  0.160209452
## 121   0.005     0.3492834     1.0217386 0.1692910 0.8307090  0.179992396
## 122   0.005     0.2941485     0.4156793 0.4411126 0.5588874 -0.146964101
## 123   0.005     0.3360497     0.7852656 0.2881953 0.7118047  0.047854387
## 124   0.005     0.3365537     1.0754677 0.1326228 0.8673772  0.203930836
## 125   0.005     0.3662285     0.8615194 0.2634129 0.7365871  0.102815611
## 126   0.005     0.3461294     0.9896932 0.1862720 0.8137280  0.159857400
## 127   0.005     0.2464853     0.5698952 0.3461940 0.6538060 -0.099708672
## 128   0.005     0.2673400     0.6060147 0.3399832 0.6600168 -0.072643265
## 129   0.005     0.3289470     0.9342460 0.2095406 0.7904594  0.119406369
## 130   0.005     0.3402370     0.7953543 0.2851423 0.7148577  0.055094724
## 131   0.005     0.3632967     0.8534693 0.2662615 0.7337385  0.097035257
## 132   0.005     0.3682959     0.8673644 0.2612991 0.7387009  0.106996786
## 133   0.005     0.3489139     1.0101025 0.1758292 0.8241708  0.173084739
## 134   0.005     0.2601433     0.5815556 0.3476151 0.6523849 -0.087471857
## 135   0.005     0.2869094     0.6927203 0.3095321 0.6904679 -0.022622758
## 136   0.005     0.3212657     0.9156905 0.2161211 0.7838789  0.105144657
## 137   0.005     0.3484310     0.8153403 0.2789285 0.7210715  0.069502499
## 138   0.005     0.2353498     0.5116057 0.3671509 0.6328491 -0.131801161
## 139   0.005     0.3136050     0.7580133 0.2911896 0.7088104  0.022415441
## 140   0.005     0.3778288     0.8972227 0.2498050 0.7501950  0.128023844
## 141   0.005     0.3309608     0.5914032 0.3772893 0.6227107 -0.046328460
## 142   0.005     0.2714300     0.6513172 0.3212644 0.6787356 -0.049834333
## 143   0.005     0.2352318     0.4292537 0.4051133 0.5948867 -0.169881513
## 144   0.005     0.3301436     1.0875909 0.1228209 0.8771791  0.207322643
## 145   0.005     0.3355524     0.8098457 0.2759357 0.7240643  0.059616710
## 146   0.005     0.2515585     0.3699698 0.4415170 0.5584830 -0.189958508
## 147   0.005     0.2471471     0.5249022 0.3669953 0.6330047 -0.119848222
## 148   0.005     0.2662597     0.6025100 0.3410562 0.6589438 -0.074796495
## 149   0.005     0.3464687     1.0469075 0.1534975 0.8465025  0.192971200
## 150   0.005     0.3042270     0.4958401 0.4085807 0.5914193 -0.104353761
## 151   0.005     0.2467337     0.5225071 0.3678825 0.6321175 -0.121148829
## 152   0.005     0.2826012     0.6815713 0.3126625 0.6873375 -0.030061224
## 153   0.005     0.2542265     0.3636916 0.4458899 0.5541101 -0.191663408
## 154   0.005     0.2520426     0.3687984 0.4423254 0.5576746 -0.190282851
## 155   0.005     0.3328313     0.8033820 0.2779030 0.7220970  0.054928276
## 156   0.005     0.2366279     0.4207060 0.4098316 0.5901684 -0.173203697
## 157   0.005     0.2985588     0.8659263 0.2318833 0.7681167  0.066675523
## 158   0.005     0.3544724     0.2356881 0.5590954 0.4409046 -0.204623061
## 159   0.005     0.2493060     0.5364247 0.3628146 0.6371854 -0.113508527
## 160   0.005     0.3357384     0.2643685 0.5354258 0.4645742 -0.199687473
## 161   0.005     0.2374830     0.4162256 0.4123732 0.5876268 -0.174890218
## 162   0.005     0.3212600     0.9156771 0.2161256 0.7838744  0.105134377
## 163   0.005     0.3534219     0.8537307 0.2618291 0.7381709  0.091592833
## 164   0.005     0.3669393     0.2186374 0.5738574 0.4261426 -0.206918077
## 165   0.005     0.2956311     0.6862682 0.3166159 0.6833841 -0.020984822
## 166   0.005     0.3470662     0.9948581 0.1837439 0.8162561  0.163322309
## 167   0.005     0.2927785     0.6788463 0.3187166 0.6812834 -0.025938019
## 168   0.005     0.3217315     0.2900238 0.5156965 0.4843035 -0.193964974
## 169   0.005     0.3465329     0.8363983 0.2675780 0.7324220  0.078954933
## 170   0.005     0.2579884     0.5736079 0.3501752 0.6498248 -0.092186840
## 171   0.005     0.3419172     0.6232174 0.3677176 0.6322824 -0.025800321
## 172   0.005     0.2944450     0.4242937 0.4371916 0.5628084 -0.142746624
## 173   0.005     0.2617331     0.6228116 0.3296128 0.6703872 -0.067879623
## 174   0.005     0.2497190     0.5384766 0.3620846 0.6379154 -0.112365600
## 175   0.005     0.2384172     0.5319449 0.3594210 0.6405790 -0.121003800
## 176   0.005     0.2329674     0.4855679 0.3778689 0.6221311 -0.144901515
## 177   0.005     0.3476308     0.8391150 0.2666940 0.7333060  0.080936819
## 178   0.005     0.2497154     0.3745795 0.4383698 0.5616302 -0.188654432
## 179   0.005     0.2763428     0.6648954 0.3173791 0.6826209 -0.041036316
## 180   0.005     0.3345433     0.6020431 0.3740630 0.6259370 -0.039519781
## 181   0.005     0.3056395     0.3301360 0.4879401 0.5120599 -0.182300589
## 182   0.005     0.2354028     0.5120241 0.3669866 0.6330134 -0.131583778
## 183   0.005     0.3031051     0.3388382 0.4824328 0.5175672 -0.179327701
## 184   0.005     0.3150382     0.3046158 0.5051501 0.4948499 -0.190111860
## 185   0.005     0.2324879     0.4688536 0.3853197 0.6146803 -0.152831804
## 186   0.005     0.3222763     0.5642799 0.3856793 0.6143207 -0.063402999
## 187   0.005     0.3137329     0.7583163 0.2911031 0.7088969  0.022629755
## 188   0.005     0.3859073     0.9960770 0.1988802 0.8011198  0.187027165
## 189   0.005     0.3405468     0.6193455 0.3688720 0.6311280 -0.028325153
## 190   0.005     0.2440433     0.3906990 0.4277897 0.5722103 -0.183746424
## 191   0.005     0.2726455     0.6225707 0.3349933 0.6650067 -0.062347845
## 192   0.005     0.2550661     0.5621891 0.3539334 0.6460666 -0.098867300
## 193   0.005     0.2589483     0.6140732 0.3322392 0.6677608 -0.073290868
## 194   0.005     0.3492764     1.0183475 0.1712435 0.8287565  0.178032933
## 195   0.005     0.2380853     0.5300162 0.3601326 0.6398674 -0.122047300
## 196   0.005     0.2886567     0.6679315 0.3218177 0.6781823 -0.033160977
## 197   0.005     0.3213491     0.7499898 0.2985455 0.7014545  0.022803574
## 198   0.005     0.3300678     0.7709168 0.2924586 0.7075414  0.037609228
## 199   0.005     0.3674304     0.2179894 0.5744273 0.4255727 -0.206996902
## 200   0.005     0.3857982     0.9966230 0.1985236 0.8014764  0.187274600
##         e_dual_0
## 1    0.196816619
## 2    0.195213882
## 3   -0.191748285
## 4    0.123649469
## 5   -0.154763228
## 6   -0.080692211
## 7   -0.073212860
## 8    0.200805087
## 9    0.133986843
## 10  -0.034787186
## 11   0.194352955
## 12  -0.174345888
## 13   0.129565297
## 14   0.144629656
## 15  -0.165065843
## 16   0.042036735
## 17   0.120126209
## 18  -0.151913105
## 19   0.201671452
## 20  -0.108044952
## 21  -0.202081459
## 22   0.197380536
## 23   0.159371623
## 24   0.167486377
## 25  -0.172100525
## 26  -0.183454463
## 27   0.154131441
## 28  -0.097708407
## 29   0.196740228
## 30   0.112833423
## 31   0.190924447
## 32  -0.203513711
## 33  -0.120295510
## 34  -0.076567356
## 35   0.196882423
## 36  -0.046483984
## 37  -0.183042426
## 38   0.108598078
## 39   0.081468450
## 40   0.198730410
## 41  -0.136679595
## 42   0.095920899
## 43  -0.182049309
## 44  -0.193805183
## 45   0.107995697
## 46   0.198095687
## 47   0.033563444
## 48  -0.204506052
## 49  -0.050518685
## 50   0.054784892
## 51  -0.201856253
## 52   0.101673736
## 53   0.191370762
## 54  -0.125933299
## 55   0.155689177
## 56   0.189411942
## 57   0.142293228
## 58  -0.086322022
## 59  -0.095287833
## 60   0.201169895
## 61   0.064937764
## 62   0.181863047
## 63   0.210076717
## 64  -0.138737584
## 65   0.197008486
## 66  -0.140667619
## 67  -0.095834024
## 68   0.060625911
## 69   0.075688846
## 70   0.051946108
## 71   0.032717185
## 72   0.195282058
## 73  -0.198458462
## 74  -0.165868213
## 75   0.119957504
## 76   0.188389324
## 77   0.175942701
## 78   0.038960517
## 79   0.103043140
## 80  -0.205101289
## 81  -0.180335073
## 82   0.177296971
## 83   0.200157253
## 84   0.167654051
## 85  -0.188097402
## 86  -0.096793921
## 87   0.071479842
## 88   0.177085871
## 89   0.210495814
## 90  -0.118300231
## 91   0.206008728
## 92   0.210657507
## 93   0.182929998
## 94  -0.160508683
## 95  -0.108160962
## 96   0.202644724
## 97   0.171801686
## 98   0.191127983
## 99  -0.174708170
## 100  0.032088604
## 101 -0.116780319
## 102 -0.099252091
## 103 -0.208558983
## 104  0.202142907
## 105 -0.143558752
## 106 -0.123096429
## 107 -0.145717023
## 108 -0.199068295
## 109  0.039860457
## 110 -0.067402993
## 111 -0.169493129
## 112  0.126072236
## 113 -0.176312441
## 114  0.141422860
## 115  0.159609474
## 116  0.192012043
## 117  0.163246672
## 118 -0.013970326
## 119 -0.152360185
## 120  0.173839226
## 121  0.191029577
## 122 -0.143208147
## 123  0.073460859
## 124  0.208090527
## 125  0.124932285
## 126  0.175965231
## 127 -0.083910746
## 128 -0.054002099
## 129  0.143786578
## 130  0.080496610
## 131  0.119730760
## 132  0.128663448
## 133  0.185931733
## 134 -0.070829260
## 135  0.002252466
## 136  0.131811543
## 137  0.094268830
## 138 -0.121243381
## 139  0.049202842
## 140  0.147027640
## 141 -0.031307588
## 142 -0.027418483
## 143 -0.165633070
## 144  0.210411816
## 145  0.085781419
## 146 -0.188513136
## 147 -0.108102464
## 148 -0.056433725
## 149  0.200405040
## 150 -0.095579159
## 151 -0.109610398
## 152 -0.005766226
## 153 -0.190418488
## 154 -0.188876206
## 155  0.081285043
## 156 -0.169462399
## 157  0.097809599
## 158 -0.205216470
## 159 -0.100760711
## 160 -0.200205676
## 161 -0.171401156
## 162  0.131802789
## 163  0.115559799
## 164 -0.207505224
## 165  0.002884147
## 166  0.178601984
## 167 -0.002437153
## 168 -0.194279658
## 169  0.103976287
## 170 -0.076216904
## 171 -0.009065075
## 172 -0.138514693
## 173 -0.047575666
## 174 -0.099438849
## 175 -0.108634103
## 176 -0.136563186
## 177  0.105808918
## 178 -0.187050661
## 179 -0.017725480
## 180 -0.023893887
## 181 -0.181923934
## 182 -0.120989252
## 183 -0.178729006
## 184 -0.190234090
## 185 -0.145826784
## 186 -0.050040718
## 187  0.049419410
## 188  0.194957139
## 189 -0.011782508
## 190 -0.181511285
## 191 -0.042435947
## 192 -0.083877501
## 193 -0.053687637
## 194  0.189590974
## 195 -0.109851211
## 196 -0.010250824
## 197  0.048535360
## 198  0.063375384
## 199 -0.207583273
## 200  0.195146550
## 
## $values
## $values$entropy
## [1] 400.115
## 
## $values$iterations
## [1] 96
## 
## $values$message
## [1] &quot;relative convergence (4)&quot;
## 
## 
## $tol
## [1] 1e-10
## 
## $v
##           [,1] [,2]       [,3]
## [1,] 0.2160606    0 -0.2160606
## 
## $lambda
##   (Intercept) Dcollege  Totalincome    Dunemp
## 1    962.9102 2136.313 -0.005113248 -1048.039
## 0   1189.9965 2308.253 -0.005420463 -1161.820
## 
## $checkrestrictions
## $checkrestrictions$g1
##        [,1]
##   [1,]    0
##   [2,]    0
##   [3,]    0
##   [4,]    0
##   [5,]    0
##   [6,]    0
##   [7,]    0
##   [8,]    0
##   [9,]    0
##  [10,]    0
##  [11,]    0
##  [12,]    0
##  [13,]    0
##  [14,]    0
##  [15,]    0
##  [16,]    0
##  [17,]    0
##  [18,]    0
##  [19,]    0
##  [20,]    0
##  [21,]    0
##  [22,]    0
##  [23,]    0
##  [24,]    0
##  [25,]    0
##  [26,]    0
##  [27,]    0
##  [28,]    0
##  [29,]    0
##  [30,]    0
##  [31,]    0
##  [32,]    0
##  [33,]    0
##  [34,]    0
##  [35,]    0
##  [36,]    0
##  [37,]    0
##  [38,]    0
##  [39,]    0
##  [40,]    0
##  [41,]    0
##  [42,]    0
##  [43,]    0
##  [44,]    0
##  [45,]    0
##  [46,]    0
##  [47,]    0
##  [48,]    0
##  [49,]    0
##  [50,]    0
##  [51,]    0
##  [52,]    0
##  [53,]    0
##  [54,]    0
##  [55,]    0
##  [56,]    0
##  [57,]    0
##  [58,]    0
##  [59,]    0
##  [60,]    0
##  [61,]    0
##  [62,]    0
##  [63,]    0
##  [64,]    0
##  [65,]    0
##  [66,]    0
##  [67,]    0
##  [68,]    0
##  [69,]    0
##  [70,]    0
##  [71,]    0
##  [72,]    0
##  [73,]    0
##  [74,]    0
##  [75,]    0
##  [76,]    0
##  [77,]    0
##  [78,]    0
##  [79,]    0
##  [80,]    0
##  [81,]    0
##  [82,]    0
##  [83,]    0
##  [84,]    0
##  [85,]    0
##  [86,]    0
##  [87,]    0
##  [88,]    0
##  [89,]    0
##  [90,]    0
##  [91,]    0
##  [92,]    0
##  [93,]    0
##  [94,]    0
##  [95,]    0
##  [96,]    0
##  [97,]    0
##  [98,]    0
##  [99,]    0
## [100,]    0
## [101,]    0
## [102,]    0
## [103,]    0
## [104,]    0
## [105,]    0
## [106,]    0
## [107,]    0
## [108,]    0
## [109,]    0
## [110,]    0
## [111,]    0
## [112,]    0
## [113,]    0
## [114,]    0
## [115,]    0
## [116,]    0
## [117,]    0
## [118,]    0
## [119,]    0
## [120,]    0
## [121,]    0
## [122,]    0
## [123,]    0
## [124,]    0
## [125,]    0
## [126,]    0
## [127,]    0
## [128,]    0
## [129,]    0
## [130,]    0
## [131,]    0
## [132,]    0
## [133,]    0
## [134,]    0
## [135,]    0
## [136,]    0
## [137,]    0
## [138,]    0
## [139,]    0
## [140,]    0
## [141,]    0
## [142,]    0
## [143,]    0
## [144,]    0
## [145,]    0
## [146,]    0
## [147,]    0
## [148,]    0
## [149,]    0
## [150,]    0
## [151,]    0
## [152,]    0
## [153,]    0
## [154,]    0
## [155,]    0
## [156,]    0
## [157,]    0
## [158,]    0
## [159,]    0
## [160,]    0
## [161,]    0
## [162,]    0
## [163,]    0
## [164,]    0
## [165,]    0
## [166,]    0
## [167,]    0
## [168,]    0
## [169,]    0
## [170,]    0
## [171,]    0
## [172,]    0
## [173,]    0
## [174,]    0
## [175,]    0
## [176,]    0
## [177,]    0
## [178,]    0
## [179,]    0
## [180,]    0
## [181,]    0
## [182,]    0
## [183,]    0
## [184,]    0
## [185,]    0
## [186,]    0
## [187,]    0
## [188,]    0
## [189,]    0
## [190,]    0
## [191,]    0
## [192,]    0
## [193,]    0
## [194,]    0
## [195,]    0
## [196,]    0
## [197,]    0
## [198,]    0
## [199,]    0
## [200,]    0
## 
## $checkrestrictions$g2
##        [,1] [,2]
##   [1,]    0    0
##   [2,]    0    0
##   [3,]    0    0
##   [4,]    0    0
##   [5,]    0    0
##   [6,]    0    0
##   [7,]    0    0
##   [8,]    0    0
##   [9,]    0    0
##  [10,]    0    0
##  [11,]    0    0
##  [12,]    0    0
##  [13,]    0    0
##  [14,]    0    0
##  [15,]    0    0
##  [16,]    0    0
##  [17,]    0    0
##  [18,]    0    0
##  [19,]    0    0
##  [20,]    0    0
##  [21,]    0    0
##  [22,]    0    0
##  [23,]    0    0
##  [24,]    0    0
##  [25,]    0    0
##  [26,]    0    0
##  [27,]    0    0
##  [28,]    0    0
##  [29,]    0    0
##  [30,]    0    0
##  [31,]    0    0
##  [32,]    0    0
##  [33,]    0    0
##  [34,]    0    0
##  [35,]    0    0
##  [36,]    0    0
##  [37,]    0    0
##  [38,]    0    0
##  [39,]    0    0
##  [40,]    0    0
##  [41,]    0    0
##  [42,]    0    0
##  [43,]    0    0
##  [44,]    0    0
##  [45,]    0    0
##  [46,]    0    0
##  [47,]    0    0
##  [48,]    0    0
##  [49,]    0    0
##  [50,]    0    0
##  [51,]    0    0
##  [52,]    0    0
##  [53,]    0    0
##  [54,]    0    0
##  [55,]    0    0
##  [56,]    0    0
##  [57,]    0    0
##  [58,]    0    0
##  [59,]    0    0
##  [60,]    0    0
##  [61,]    0    0
##  [62,]    0    0
##  [63,]    0    0
##  [64,]    0    0
##  [65,]    0    0
##  [66,]    0    0
##  [67,]    0    0
##  [68,]    0    0
##  [69,]    0    0
##  [70,]    0    0
##  [71,]    0    0
##  [72,]    0    0
##  [73,]    0    0
##  [74,]    0    0
##  [75,]    0    0
##  [76,]    0    0
##  [77,]    0    0
##  [78,]    0    0
##  [79,]    0    0
##  [80,]    0    0
##  [81,]    0    0
##  [82,]    0    0
##  [83,]    0    0
##  [84,]    0    0
##  [85,]    0    0
##  [86,]    0    0
##  [87,]    0    0
##  [88,]    0    0
##  [89,]    0    0
##  [90,]    0    0
##  [91,]    0    0
##  [92,]    0    0
##  [93,]    0    0
##  [94,]    0    0
##  [95,]    0    0
##  [96,]    0    0
##  [97,]    0    0
##  [98,]    0    0
##  [99,]    0    0
## [100,]    0    0
## [101,]    0    0
## [102,]    0    0
## [103,]    0    0
## [104,]    0    0
## [105,]    0    0
## [106,]    0    0
## [107,]    0    0
## [108,]    0    0
## [109,]    0    0
## [110,]    0    0
## [111,]    0    0
## [112,]    0    0
## [113,]    0    0
## [114,]    0    0
## [115,]    0    0
## [116,]    0    0
## [117,]    0    0
## [118,]    0    0
## [119,]    0    0
## [120,]    0    0
## [121,]    0    0
## [122,]    0    0
## [123,]    0    0
## [124,]    0    0
## [125,]    0    0
## [126,]    0    0
## [127,]    0    0
## [128,]    0    0
## [129,]    0    0
## [130,]    0    0
## [131,]    0    0
## [132,]    0    0
## [133,]    0    0
## [134,]    0    0
## [135,]    0    0
## [136,]    0    0
## [137,]    0    0
## [138,]    0    0
## [139,]    0    0
## [140,]    0    0
## [141,]    0    0
## [142,]    0    0
## [143,]    0    0
## [144,]    0    0
## [145,]    0    0
## [146,]    0    0
## [147,]    0    0
## [148,]    0    0
## [149,]    0    0
## [150,]    0    0
## [151,]    0    0
## [152,]    0    0
## [153,]    0    0
## [154,]    0    0
## [155,]    0    0
## [156,]    0    0
## [157,]    0    0
## [158,]    0    0
## [159,]    0    0
## [160,]    0    0
## [161,]    0    0
## [162,]    0    0
## [163,]    0    0
## [164,]    0    0
## [165,]    0    0
## [166,]    0    0
## [167,]    0    0
## [168,]    0    0
## [169,]    0    0
## [170,]    0    0
## [171,]    0    0
## [172,]    0    0
## [173,]    0    0
## [174,]    0    0
## [175,]    0    0
## [176,]    0    0
## [177,]    0    0
## [178,]    0    0
## [179,]    0    0
## [180,]    0    0
## [181,]    0    0
## [182,]    0    0
## [183,]    0    0
## [184,]    0    0
## [185,]    0    0
## [186,]    0    0
## [187,]    0    0
## [188,]    0    0
## [189,]    0    0
## [190,]    0    0
## [191,]    0    0
## [192,]    0    0
## [193,]    0    0
## [194,]    0    0
## [195,]    0    0
## [196,]    0    0
## [197,]    0    0
## [198,]    0    0
## [199,]    0    0
## [200,]    0    0
## 
## $checkrestrictions$g3
##                 1     0
## (Intercept) 0.000 0.000
## Dcollege    0.000 0.000
## Totalincome 0.063 0.078
## Dunemp      0.000 0.000
## 
## 
## $cross_moments_hp
##                    1         0
## (Intercept)     0.31      0.69
## Dcollege        0.17      0.45
## Totalincome 94640.65 197483.48
## Dunemp          0.15      0.28
## 
## $cross_moments_hs
##             1          0         
## (Intercept)       0.31       0.69
## Dcollege          0.17       0.45
## Totalincome   94640.59  197483.40
## Dunemp            0.15       0.28
## 
## $J
## [1] 2
## 
## $fn
## datahp$poor_liq ~ Dcollege + Totalincome + Dunemp
## 
## $divergencekl
## [1] 19.84858
## 
## attr(,&quot;class&quot;)
## [1] &quot;shannon&quot;</code></pre>
<p>To make the results more visual, this package includes a personalized
summary function which will resume the main results, providing the means
for each category <span class="math inline">\(j\)</span> for the
predictions, the probabilities and the error.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="at">object=</span>result3)</span></code></pre></div>
<pre><code>## Iterations[1] 96
## Entropy value[1] 400.115
## [1] 19.84858
## [1] &quot;mean_estimations&quot;
##   weights predictions_1 predictions_0  p_dual_1  p_dual_0     e_dual_1
## 1   0.005     0.3099998     0.6899999 0.3166901 0.6833099 -0.006690374
##     e_dual_0
## 1 0.00669001
## [1] &quot;lambda&quot;
##   (Intercept) Dcollege  Totalincome    Dunemp
## 1    962.9102 2136.313 -0.005113248 -1048.039
## 0   1189.9965 2308.253 -0.005420463 -1161.820</code></pre>
<p>Graphs are generated with the plot function, showing the averages of
the predictions for each territorial unit and the 95% confidence
interval associated with each of them.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span>result3, <span class="at">reg=</span>datahs<span class="sc">$</span>reg)   </span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --><img role="img" src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Fernandez-Vazquez, E., Díaz-Dapena, A., Rubiera-Morollon, F.,
Viñuela, A., (2020) Spatial Disaggregation of Social Indicators: An
Info-Metrics Approach. Social Indicators Research, 152(2), 809–821. <a href="https://doi.org/10.1007/s11205-020-02455-z" class="uri">https://doi.org/10.1007/s11205-020-02455-z</a>.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
